{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline : Food-drug Interaction Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load food names or compounds into a list of unique items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import json\n",
    "import jellyfish\n",
    "\n",
    "# UTF-8 support\n",
    "import codecs\n",
    "\n",
    "# To use PubMed API\n",
    "import pubmed.utils as pb\n",
    "\n",
    "# Split abstracts to sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceNonASCII(text):\n",
    "    ''' Replace all non ASCII characters by 'unk'\n",
    "        This is to deal with Unicode encode/decode bug when using PySpark\n",
    "    '''\n",
    "    if text is not None:\n",
    "        return ''.join([i if ord(i) < 128 else 'unk' for i in text])\n",
    "\n",
    "    \n",
    "def list_loader(data):\n",
    "    ''' Load pickle file\n",
    "        Returns keys of the loaded dictionary as a set\n",
    "    '''\n",
    "    test = pickle.load(open( data , \"rb\"))\n",
    "    foodlist = []\n",
    "    for food in test.keys():\n",
    "        food = replaceNonASCII(food.decode('utf-8').lower())\n",
    "        foodlist.append(food)\n",
    "        \n",
    "    return set(foodlist)  # Will work with a set rather than a list. Faster search for later (hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Adam's pickle file (food common name as sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pickling the data from foodb.ca database, see Adam's notebook **compound_food_id.ipynb**  \n",
    "\n",
    "As a first test, we will use only the food common name (not scientific name) only. Compounds names will be added once this test passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# food_common.pickle: Dictionary with common English food names as keys, compounds as values\n",
    "test = pickle.load(open( \"data/food_common.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food name\n",
      "----------\n",
      "Oregon yampah\n",
      "Okra\n",
      "Black mulberry\n",
      "Avocado\n",
      "Parsley\n",
      "Elderberry\n",
      "Sugar\n",
      "Sweet bay\n",
      "Common bean\n",
      "Fig\n",
      "Lard\n"
     ]
    }
   ],
   "source": [
    "# Quick peek at the loaded data\n",
    "for i, item in enumerate(test.iteritems()):\n",
    "    if i == 0:\n",
    "        print 'food name'\n",
    "        print '-'*10\n",
    "    print '{0}'.format(item[0])\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodlist = list_loader('data/food_common.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atlantic pollock\n",
      "mixed nuts\n",
      "rose hip\n",
      "black mulberry\n",
      "pheasant\n",
      "whiting\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for food in foodlist:\n",
    "    print food\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Filter sentences from abstract with drug keyword and food names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Test with only the abstracts' first json file. Once works, we can add all 100 remaining files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abstract_loader(name):\n",
    "    ''' Loads abstracts from json files in provided list\n",
    "    '''\n",
    "    data_list = []\n",
    "    for jsonfile in name:\n",
    "        with codecs.open(jsonfile,\"r\",\"utf-8\") as data_file:\n",
    "            data = json.load(data_file)\n",
    "            values = [replaceNonASCII(value) for value in data.values()]\n",
    "            data_list.extend(values)\n",
    "            \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def splitSentences(abstract):\n",
    "    sentences = sent_tokenize(abstract)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def find_ngrams(sentence, n):\n",
    "    ''' Return list of ngrams from a sentence\n",
    "    '''\n",
    "    words_list = sentence.split()\n",
    "    ngrams = zip(*[words_list[i:] for i in range(n)])\n",
    "    #return [''.join([unicode(w)+' ' for w in ngram if type(w)==unicode]).strip() for ngram in ngrams]\n",
    "    return [''.join([w+' ' for w in ngram]).strip() for ngram in ngrams]\n",
    "\n",
    "\n",
    "#######################\n",
    "# Method 1 - NOT USED #\n",
    "#######################\n",
    "\n",
    "# NOT USED since will return True if finds a food name within a word in the sentence\n",
    "# E.g.: \"pie\" food name and \"therapies\" word in sentence: return True since \"pie\" in \"therapies\"\n",
    "#def includeFoodCmpd(sentence, fdlist):\n",
    "#    if any(word in sentence for word in fdlist):\n",
    "#        return True\n",
    "#    else:\n",
    "#        return False\n",
    "\n",
    "\n",
    "###################################\n",
    "# Method 2 - solution to method 2 #\n",
    "###################################\n",
    "\n",
    "def includeFoodCmpd(sentence, fdlist, limit, verbose=False):\n",
    "    ''' Calculates the Jaro Wrinkler distance between food name and ngrams in the sentence.\n",
    "        Returns True if distance > 0.95\n",
    "    '''\n",
    "    result = False\n",
    "    for food in fdlist:\n",
    "        n = min(3, len(food.split()))  # Assuming max as trigram        \n",
    "        sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                # last word. For now ok, since the JW will still be > 0.95\n",
    "        for ngram in sentence_ngrams:\n",
    "            # Note: when using jaro_winkler, need to convert into unicode format\n",
    "            ngram_distance = jellyfish.jaro_winkler(food.lower(), u\"{}\".format(ngram.lower()))\n",
    "            if verbose:\n",
    "                print food, ngram, ngram_distance\n",
    "            if ngram_distance > limit:  \n",
    "                result = True\n",
    "                break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def findFood(sentence, foods):\n",
    "    ''' Returns the food names found in the sentence based on string distance method\n",
    "    '''\n",
    "    \n",
    "    food = []\n",
    "\n",
    "    # If using string distance method:\n",
    "    for f in foods:\n",
    "        n = min(3, len(f.split()))  # Assuming max as trigram        \n",
    "        sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                # last word. For now ok, since the JW will still be > 0.95\n",
    "        for ngram in sentence_ngrams:\n",
    "            # Note: when using jaro_winkler, need to convert into unicode format\n",
    "            if jellyfish.jaro_winkler(f.lower(), u\"{}\".format(ngram.lower())) > 0.95:\n",
    "                food.append(f)\n",
    "            \n",
    "    return food\n",
    "\n",
    "\n",
    "# Not used since we will use the string distance for filtering\n",
    "def findFoodItems(sentence, foods):\n",
    "    for item in foods:\n",
    "        if item in sentence:\n",
    "            print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'We are working',\n",
       " u'are working hard',\n",
       " u'working hard on',\n",
       " u'hard on 266',\n",
       " u'on 266 project',\n",
       " u'266 project baseline']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test find_ngrams:\n",
    "string = u\"We are working hard on 266 project baseline\"\n",
    "find_ngrams(string, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello Panda 0.0\n",
      "hello is 0.0\n",
      "hello eating 0.455555555556\n",
      "hello a 0.0\n",
      "hello pie 0.511111111111\n",
      "pie Panda 0.511111111111\n",
      "pie is 0.0\n",
      "pie eating 0.5\n",
      "pie a 0.0\n",
      "pie pie 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing includeFoodCmpd function\n",
    "ss = u'Panda is eating a pie'\n",
    "testlist = [u\"hello\",u\"pie\"]\n",
    "includeFoodCmpd(ss, testlist, 0.98, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all abstracts - there are 101 json files from PubMed\n",
    "filename_list = []\n",
    "for i in xrange(1, 102):\n",
    "    filename_temp = \"pbabstract\" + str(i)+ \".json\"\n",
    "    filename_list.append(filename_temp)\n",
    "\n",
    "abstract = abstract_loader(filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Heart failure still has a significant disease burden with poor outcomes worldwide despite advances in therapy. The standard therapies have been focused on blockade of renin-angiotensin-aldosterone system with angiotensin-converting enzyme inhibitors, angiotensin receptor blockers and mineralocorticoid antagonists and the sympathetic nervous system with unk-blockers. The natriuretic peptide system is a potential counter-regulatory system that promotes vasodilatation and natriuresis. Angiotensin receptor neprilysin inhibitors are a new class drug capable of blocking the renin-angiotensin-aldosterone system and enhancing the natriuretic peptide system to improve neurohormonal balance. The success of the PARADIGM-HF trial with LCZ696 and its approval for heart failure treatment is likely to generate a paradigm shift. This review summarises the current knowledge of LCZ696 with a focus on pharmacology, pharmacokinetics and pharmacodynamics, mechanisms of action, clinical efficacy and safety.',\n",
       " u'All patients with lower extremity peripheral arterial disease (LE-PAD) should benefit from recommended pharmacologic therapies including antiplatelet agents, angiotensin-converting enzyme (ACE) inhibitors, or angiotensin receptor blockers (ARBs), and hydroxy-methyl-glutaryl-coenzyme A reductase inhibitors (statins). In the present study, this triple therapy was defined as the best medical treatment. This study was designed to determine the number of patients who received best medical treatment at admission and at discharge from a vascular surgery department. We also examined the number of patients who received adapted medical treatment and every pharmacologic class separately. Finally, we investigated whether there were differences in prescribing rates according to patient characteristics and cardiovascular history, clinical grade of LE-PAD, and the type of surgery practiced.']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstractRDD = sc.parallelize(abstract)\\\n",
    "                .filter(lambda a: a is not None)  #Some lines were empty: need to filter out\n",
    "abstractRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: tried to braodcast the foodlist but got an error message when used it in below filter\n",
    "# \"TypeError: 'Broadcast' object is not iterable\".... any idea why?\n",
    "# foodlist_bcast = sc.broadcast(foodlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drugkeyword = \"ACEI\"\n",
    "\n",
    "sentences = abstractRDD.flatMap(splitSentences) \\\n",
    "                       .map(lambda a: pb.ace_substitutor(a, drugkeyword)) \\\n",
    "                       .filter(lambda a: drugkeyword in a)\\\n",
    "                       .filter(lambda a: includeFoodCmpd(a, foodlist, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.',\n",
       " u'digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.2 ms, sys: 30.3 ms, total: 101 ms\n",
      "Wall time: 21min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filtered_sentences = sentences.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered sentences containing drug and food name in textfile\n",
    "with open(\"data/FilteredSentences.txt\", \"w\") as outcomes:\n",
    "    for sentence in filtered_sentences:\n",
    "        food = findFood(sentence, foodlist)\n",
    "        outcomes.writelines(str(food) + sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Classify sentences as positive or negative based on a sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 938 sentences containing both ACE Inhibitor drug and a food name from Step 2. They will be labeled and then compared to the 2 baseline models classification for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE 1: 50/50 coin flip classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model is to simply classify the interaction between drug and food in the sentence as positive, negative or neutral based on equal probability of 1/3 in each class.  \n",
    "\n",
    "Evaluation metrics: the labels of 100 randomly chosen sentences from outcomes in step 2 (i.e. sentences with drug and food names) will be compared with the random labels from baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classification = [(random.choice([\"positive\", \"negative\", \"neutral\"]), sentence) for sentence in filtered_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE 2: Sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment lexicon used is the Harvard General Inquirer (http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm). It contains 1,915 positive words and 2,291 negative words and is free for research use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (63,108,109,110,176) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "db_sentiment = pd.read_csv(\"data/inquirerbasic.csv\", encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Source</th>\n",
       "      <th>Positiv</th>\n",
       "      <th>Negativ</th>\n",
       "      <th>Pstv</th>\n",
       "      <th>Affil</th>\n",
       "      <th>Ngtv</th>\n",
       "      <th>Hostile</th>\n",
       "      <th>Strong</th>\n",
       "      <th>Power</th>\n",
       "      <th>...</th>\n",
       "      <th>Anomie</th>\n",
       "      <th>NegAff</th>\n",
       "      <th>PosAff</th>\n",
       "      <th>SureLw</th>\n",
       "      <th>If</th>\n",
       "      <th>NotLw</th>\n",
       "      <th>TimeSpc</th>\n",
       "      <th>FormLw</th>\n",
       "      <th>Othtags</th>\n",
       "      <th>Defined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DET ART</td>\n",
       "      <td>| article: Indefinite singular article--some o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ngtv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>H4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noun</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABATE</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABATEMENT</td>\n",
       "      <td>Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entry Source Positiv  Negativ Pstv Affil  Ngtv Hostile Strong Power  \\\n",
       "0            A  H4Lvd     NaN      NaN  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "1      ABANDON  H4Lvd     NaN  Negativ  NaN   NaN  Ngtv     NaN    NaN   NaN   \n",
       "2  ABANDONMENT     H4     NaN  Negativ  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "3        ABATE  H4Lvd     NaN  Negativ  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "4    ABATEMENT    Lvd     NaN      NaN  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "\n",
       "                         ...                         Anomie NegAff PosAff  \\\n",
       "0                        ...                            NaN    NaN    NaN   \n",
       "1                        ...                            NaN    NaN    NaN   \n",
       "2                        ...                            NaN    NaN    NaN   \n",
       "3                        ...                            NaN    NaN    NaN   \n",
       "4                        ...                            NaN    NaN    NaN   \n",
       "\n",
       "  SureLw   If NotLw TimeSpc FormLw  Othtags  \\\n",
       "0    NaN  NaN   NaN     NaN    NaN  DET ART   \n",
       "1    NaN  NaN   NaN     NaN    NaN     SUPV   \n",
       "2    NaN  NaN   NaN     NaN    NaN     Noun   \n",
       "3    NaN  NaN   NaN     NaN    NaN     SUPV   \n",
       "4    NaN  NaN   NaN     NaN    NaN     Noun   \n",
       "\n",
       "                                             Defined  \n",
       "0  | article: Indefinite singular article--some o...  \n",
       "1                                                  |  \n",
       "2                                                  |  \n",
       "3                                                  |  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 186 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry\n",
      "Source\n",
      "Positiv\n",
      "Negativ\n",
      "Pstv\n",
      "Affil\n",
      "Ngtv\n",
      "Hostile\n",
      "Strong\n",
      "Power\n",
      "Weak\n",
      "Submit\n",
      "Active\n",
      "Passive\n",
      "Pleasur\n",
      "Pain\n",
      "Feel\n",
      "Arousal\n",
      "EMOT\n",
      "Virtue\n",
      "Vice\n",
      "Ovrst\n",
      "Undrst\n",
      "Academ\n",
      "Doctrin\n",
      "Econ@\n",
      "Exch\n",
      "ECON\n",
      "Exprsv\n",
      "Legal\n",
      "Milit\n",
      "Polit@\n",
      "POLIT\n",
      "Relig\n",
      "Role\n",
      "COLL\n",
      "Work\n",
      "Ritual\n",
      "SocRel\n",
      "Race\n",
      "Kin@\n",
      "MALE\n",
      "Female\n",
      "Nonadlt\n",
      "HU\n",
      "ANI\n",
      "PLACE\n",
      "Social\n",
      "Region\n",
      "Route\n",
      "Aquatic\n",
      "Land\n",
      "Sky\n",
      "Object\n",
      "Tool\n",
      "Food\n",
      "Vehicle\n",
      "BldgPt\n",
      "ComnObj\n",
      "NatObj\n",
      "BodyPt\n",
      "ComForm\n",
      "COM\n",
      "Say\n",
      "Need\n",
      "Goal\n",
      "Try\n",
      "Means\n",
      "Persist\n",
      "Complet\n",
      "Fail\n",
      "NatrPro\n",
      "Begin\n",
      "Vary\n",
      "Increas\n",
      "Decreas\n",
      "Finish\n",
      "Stay\n",
      "Rise\n",
      "Exert\n",
      "Fetch\n",
      "Travel\n",
      "Fall\n",
      "Think\n",
      "Know\n",
      "Causal\n",
      "Ought\n",
      "Perceiv\n",
      "Compare\n",
      "Eval@\n",
      "EVAL\n",
      "Solve\n",
      "Abs@\n",
      "ABS\n",
      "Quality\n",
      "Quan\n",
      "NUMB\n",
      "ORD\n",
      "CARD\n",
      "FREQ\n",
      "DIST\n",
      "Time@\n",
      "TIME\n",
      "Space\n",
      "POS\n",
      "DIM\n",
      "Rel\n",
      "COLOR\n",
      "Self\n",
      "Our\n",
      "You\n",
      "Name\n",
      "Yes\n",
      "No\n",
      "Negate\n",
      "Intrj\n",
      "IAV\n",
      "DAV\n",
      "SV\n",
      "IPadj\n",
      "IndAdj\n",
      "PowGain\n",
      "PowLoss\n",
      "PowEnds\n",
      "PowAren\n",
      "PowCon\n",
      "PowCoop\n",
      "PowAuPt\n",
      "PowPt\n",
      "PowDoct\n",
      "PowAuth\n",
      "PowOth\n",
      "PowTot\n",
      "RcEthic\n",
      "RcRelig\n",
      "RcGain\n",
      "RcLoss\n",
      "RcEnds\n",
      "RcTot\n",
      "RspGain\n",
      "RspLoss\n",
      "RspOth\n",
      "RspTot\n",
      "AffGain\n",
      "AffLoss\n",
      "AffPt\n",
      "AffOth\n",
      "AffTot\n",
      "WltPt\n",
      "WltTran\n",
      "WltOth\n",
      "WltTot\n",
      "WlbGain\n",
      "WlbLoss\n",
      "WlbPhys\n",
      "WlbPsyc\n",
      "WlbPt\n",
      "WlbTot\n",
      "EnlGain\n",
      "EnlLoss\n",
      "EnlEnds\n",
      "EnlPt\n",
      "EnlOth\n",
      "EnlTot\n",
      "SklAsth\n",
      "SklPt\n",
      "SklOth\n",
      "SklTot\n",
      "TrnGain\n",
      "TrnLoss\n",
      "TranLw\n",
      "MeansLw\n",
      "EndsLw\n",
      "ArenaLw\n",
      "PtLw\n",
      "Nation\n",
      "Anomie\n",
      "NegAff\n",
      "PosAff\n",
      "SureLw\n",
      "If\n",
      "NotLw\n",
      "TimeSpc\n",
      "FormLw\n",
      "Othtags\n",
      "Defined\n"
     ]
    }
   ],
   "source": [
    "# Look at all data fields available\n",
    "for column in db_sentiment.columns:\n",
    "    print column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** some columns seem quite interesting for analysing relationship other than simply positive or negative sentiment (e.g. \"causal\", etc.). For the baseline, we will only use the \"positive\" and \"negative\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter only the words labeled positive or negative\n",
    "positive = db_sentiment[db_sentiment.Positiv == \"Positiv\"].Entry.map(lambda x: x.lower()).tolist()\n",
    "negative = db_sentiment[db_sentiment.Negativ == \"Negativ\"].Entry.map(lambda x: x.lower()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abide', u'ability', u'able', u'abound']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abandon', u'abandonment', u'abate', u'abdicate']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform list into sets for faster search\n",
    "positive = set(positive)\n",
    "negative = set(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an attempt to classify whether a sentence is positive or negative.  \n",
    "Note the main weaknesses:  \n",
    "1. It is \"positively\" biased for now since looks at the positive words first and if it finds it, then it immediately returns positive. Thus, it may not look at the entire sentence in case of both positive or negative words.  \n",
    "2. Negation of a positive word is not taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def includeSentiment(sentence, poslist, neglist): \n",
    "    ''' Classify sentence as positive or negative based on first word found\n",
    "        in the lexicon\n",
    "    '''\n",
    "    if any(word in sentence for word in poslist):\n",
    "        return (\"positive\", sentence)\n",
    "    elif any(word in sentence for word in neglist):\n",
    "        return (\"negative\", sentence)\n",
    "    else:\n",
    "        return (\"neutral\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add sentiment as key in the RDD\n",
    "\n",
    "filtered_sentencesRDD = sc.parallelize(filtered_sentences)\n",
    "sentiments = filtered_sentencesRDD.map(lambda a: includeSentiment(a, positive, negative))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positive',\n",
       "  u'in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.'),\n",
       " ('positive',\n",
       "  u'digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.')]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at 2 first lines\n",
    "sentiments.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 2 lists of filtered sentences: one positive list and one negative list\n",
    "pos_sentiments = sentiments.lookup(\"positive\")\n",
    "neg_sentiments = sentiments.lookup(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.',\n",
       " u'digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.',\n",
       " u'the peptides from whey and caseins exert significant ACEI inhibitory activities comparable to that of captopril, an antihypertensive drug, exhibiting ic50 values of 4.45unkunkm and 4.27unkunkm, respectively.',\n",
       " u'the results introduce, for the first time, new potent ACEI-inhibitory peptides that can be released by gastric pepsin of goat milk whey and caseins and thus may pave the way for their candidacy as anti-hypertensive bioactive peptides and prevention of associated disorders.',\n",
       " u'ACEI use between 1 january 2003 and the index date were determined by the date of hospitalization for acute pancreatitis among the cases.',\n",
       " u'compared to hydrolysates from whey protein, where the inhibitory effect can almost exclusively be attributed to ile-trp, the ACEI inhibition by plant protein hydrolysates is caused by a variety of peptides, in particular tyrosine-containing peptides.',\n",
       " u'the annual rate of adverse events related to ACEI (ie, the number of reported cases of adverse events per 1000 patients receiving an ACEI) was calculated from data captured on the date the events were first reported for the 5 years before and 5 years after the revocation of the pa constraint.',\n",
       " u'these findings suggest a therapeutic potential for mas and/or at2 receptor activation and ACEI inhibition in restoring endothelial function impaired by elevated dietary salt intake or other pathological conditions.',\n",
       " u'casein haplotype significantly influenced the antioxidative and ACEI-inhibitory capacities of digested casein.',\n",
       " u'in particular, bb-a(2)a(1)-aa casein and bb-a(1)a(1)-aa casein showed the highest ACEI-inhibitory capacity, bb-a(2)a(2)-aa casein showed the highest antioxidant capacity, whereas bb-a(2)a(2)-bb casein showed the lowest biological capacity.']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of sentences with positive sentiment lexicon\n",
    "pos_sentiments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the late-eluting fraction (f4) of either whey or caseins exhibited greater ACEI inhibition.',\n",
       " u'we investigated the molecular mechanisms involved in the ACEI (ACEI) inhibition by (-)-epigallocatechin-3-gallate (egcg), a major tea catechin.']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of sentences with negative sentiment lexicon\n",
    "neg_sentiments[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findSentiment(sentence, sentiment, poslist, neglist):\n",
    "    '''Print out the lexicon word that classified the sentence as positive or negative\n",
    "    '''\n",
    "    if sentiment == \"positive\":\n",
    "        lexicon = poslist\n",
    "    else:\n",
    "        lexicon = neglist\n",
    "    for word in lexicon:\n",
    "        # Note: had to use this \"try/except\" since there was an unicode ascii error... any ways\n",
    "        # to fix this without this try/except? if left the same, then we won't be able to see\n",
    "        # some sentiment word in some sentences.\n",
    "        try:\n",
    "            if word in sentence:\n",
    "                print word\n",
    "        except:\n",
    "            next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'casein', u'whey']\n",
      "pro\n",
      "generate\n",
      "significant\n"
     ]
    }
   ],
   "source": [
    "s5 = u\"digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins\"\n",
    "print findFood(s5, foodlist)\n",
    "findSentiment(s5, \"positive\", positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'casein', u'whey']\n",
      "inhibit\n",
      "inhibition\n"
     ]
    }
   ],
   "source": [
    "s6 = u\"the late-eluting fraction (f4) of either whey or caseins exhibited greater ACEI inhibition\"\n",
    "print findFood(s6, foodlist)\n",
    "findSentiment(s6, \"negative\", positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTags(sentence, sentiment, foods, poslist, neglist, limit):\n",
    "    ''' Returns the tags of the sentence \n",
    "        Both lexicon word that classified the sentence as positive or negative and food name\n",
    "    '''\n",
    "    \n",
    "    sent = []\n",
    "    food = []\n",
    "    \n",
    "    if sentiment == \"positive\":\n",
    "        lexicon = poslist\n",
    "    else:\n",
    "        lexicon = neglist\n",
    "    for word in lexicon:\n",
    "        # Note: had to use this \"try/except\" since there was an unicode ascii error... any ways\n",
    "        # to fix this without this try/except? if left the same, then we won't be able to see\n",
    "        # some sentiment word in some sentences.\n",
    "        try:\n",
    "            if word in sentence:\n",
    "                sent.append(word)\n",
    "        except:\n",
    "            next\n",
    "            \n",
    "    # If using simple test of if food name \"in\" sentence method\n",
    "    #for f in foods:\n",
    "    #    if f in sentence:\n",
    "    #        food.append(f)\n",
    "    \n",
    "    # If using string distance method:\n",
    "    for f in foods:\n",
    "        n = min(3, len(f.split()))  # Assuming max as trigram        \n",
    "        sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                # last word. For now ok, since the JW will still be > 0.95\n",
    "        for ngram in sentence_ngrams:\n",
    "            # Note: when using jaro_winkler, need to convert into unicode format\n",
    "            if jellyfish.jaro_winkler(f.lower(), u\"{}\".format(ngram.lower())) > limit:\n",
    "                food.append(f)\n",
    "\n",
    "            \n",
    "    return [sent, food, sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save results in a text file\n",
    "# Note: this could have been also done in Spark! But felt lazy to code... feel free to try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/Positive.txt\", \"w\") as pos:\n",
    "    for sentence in pos_sentiments:\n",
    "        tags = findTags(sentence, \"positive\", foodlist, positive, negative, 0.95)\n",
    "        pos.writelines(str(tags)+ \"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/Negative.txt\", \"w\") as pos:\n",
    "    for sentence in pos_sentiments:\n",
    "        tags = findTags(sentence, \"negative\", foodlist, positive, negative, 0.95)\n",
    "        pos.writelines(str(tags)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes**  \n",
    "1. In our baseline, some words like \"date\" that appear in a sentence will be interpreted as the fruit \"date\" instead of a calendar date and thus, will be filtered as outputs sentences. This can only be solved if we take into account the context of the sentence and we will need ML to model this!  \n",
    "2. Sentiment analysis need a major improvement: only basing on the positive and negative words without how the food and drug are connected through these words is not a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
