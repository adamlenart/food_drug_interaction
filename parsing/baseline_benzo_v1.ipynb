{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filtering : Food-drug Interaction Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drug: Benzodiazopene\n",
    "## Food: common food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load food names or compounds into a list of unique items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import json\n",
    "import jellyfish\n",
    "\n",
    "# UTF-8 support\n",
    "import codecs\n",
    "\n",
    "# To use PubMed API\n",
    "import pubmed.utils as pb\n",
    "\n",
    "# Split abstracts to sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceNonASCII(text):\n",
    "    ''' Replace all non ASCII characters by 'unk'\n",
    "        This is to deal with Unicode encode/decode bug when using PySpark\n",
    "    '''\n",
    "    if text is not None:\n",
    "        return ''.join([i if ord(i) < 128 else 'unk' for i in text])\n",
    "\n",
    "    \n",
    "def list_loader(data):\n",
    "    ''' Load pickle file\n",
    "        Returns keys of the loaded dictionary as a set\n",
    "    '''\n",
    "    test = pickle.load(open( data , \"rb\"))\n",
    "    foodlist = []\n",
    "    for food in test.keys():\n",
    "        food = replaceNonASCII(food.decode('utf-8').lower())\n",
    "        foodlist.append(food)\n",
    "        \n",
    "    return set(foodlist)  # Will work with a set rather than a list. Faster search for later (hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Adam's pickle file (food common name as sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pickling the data from foodb.ca database, see Adam's notebook **compound_food_id.ipynb**  \n",
    "\n",
    "As a first test, we will use only the food common name (not scientific name) only. Compounds names will be added once this test passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# food_common.pickle: Dictionary with common English food names as keys, compounds as values\n",
    "test = pickle.load(open( \"data/food_common.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food name\n",
      "----------\n",
      "Oregon yampah\n",
      "Okra\n",
      "Black mulberry\n",
      "Avocado\n",
      "Parsley\n",
      "Elderberry\n",
      "Sugar\n",
      "Sweet bay\n",
      "Common bean\n",
      "Fig\n",
      "Lard\n"
     ]
    }
   ],
   "source": [
    "# Quick peek at the loaded data\n",
    "for i, item in enumerate(test.iteritems()):\n",
    "    if i == 0:\n",
    "        print 'food name'\n",
    "        print '-'*10\n",
    "    print '{0}'.format(item[0])\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foodlist = list_loader('data/food_common.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atlantic pollock\n",
      "mixed nuts\n",
      "rose hip\n",
      "black mulberry\n",
      "pheasant\n",
      "whiting\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for food in foodlist:\n",
    "    print food\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create list of words for drug keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drug_options = ['benzodiazepine', 'benzodiazepines', 'BZD', 'BZs', '(BZD)', '(BZs)','benzos', \n",
    "                'benzodiazepine-', 'benzodiazepines-', '-benzodiazepine', '-benzodiazepines',\n",
    "                \"Thienalprazolam\",\"Tetrazepam\",\"Temazepam\",\"Rilmazafone\",\"Quazepam\",\"Pyrazolam\",\n",
    "                \"Premazepam\",\"Prazepam\",\"Pinazepam\",\"Phenazepam\",\"Oxazepam\",\"Nordiazepam\",\"Nitrazepam\",\n",
    "                \"Nimetazepam\",\"Nifoxipam\",\"Midazolam\",\"Mexazolam\",\"Medazepam\",\"Meclonazepam\",\"Lormetazepam\",\n",
    "                \"Lorazepam\",\"Loprazolam\",\"Ketazolam\",\"Halazepam\",\"Flutoprazepam\",\"Flutazolam\",\"Flurazepam\",\n",
    "                \"Flunitrazepam\",\"Flubromazolam\",\"Flubromazepam\",\"Etizolam\",\"Ethyl loflazepate\",\n",
    "                \"Ethyl carfluzepate\",\"Estazolam\",\"Diclazepam\",\"Diazepam\",\"Deschloroetizolam\",\"Delorazepam\",\n",
    "                \"Cloxazolam\",\"Clotiazepam\",\"Clorazepate\",\"Clonazolam\",\"Clonazepam\",\"Clobazam\",\"Cinolazepam\",\n",
    "                \"Cinazepam\",\"Chlordiazepoxide\",\"Camazepam\",\"Brotizolam\",\"Bromazepam\",\"Bretazenil\",\"Bentazepam\",\n",
    "                \"Alprazolam\",\"Adinazolam\",\"Alprazolam\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Filter sentences from abstract with drug keyword and food names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Test with only the abstracts' first json file. Once works, we can add all 100 remaining files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = ['pbabstract1.json']\n",
    "data_list = []\n",
    "for jsonfile in name:\n",
    "    with open('benzo_pbabstract/'+jsonfile, \"r\") as data_file:\n",
    "        data = json.load(data_file)\n",
    "        values = [replaceNonASCII(value) for value in data.values()]\n",
    "        data_list.extend(values)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Next-day residual effects are a common problem with current hypnotics. The purpose of the present study was to evaluate the residual effects of eszopiclone on the physical and cognitive functions of healthy elderly people in the early morning and the day following drug administration. Four men and six women aged 63-72unkyears were administered eszopiclone 1unkmg or placebo in a randomized, double-blind and crossover design. Measures of objective parameters and subjective ratings were obtained at 4:00, 6:00, and every 2unkh from 6:00 to 16:00unkhours. For the timed up-and-go test, the main effects of time were seen. For the critical flicker fusion, eszopiclone had significantly worse results compared to placebo in early morning (4:00). There were no significant differences between eszopiclone and placebo in other objective assessments. For the sleep latency, eszopiclone had significantly shorter results compared to placebo (eszopiclone vs placebounk=unk28.4 vs 52.5unkmin, punk=unk0.047). Feeling of deep sleep and the number of wake after sleep onset did not show any significant differences between eszopiclone and placebo. Based on the above results, the changes of physical and cognitive functions in the healthy elderly after taking hypnotics, it was found that eszopiclone 1unkmg is likely to be unharmful for the healthy elderly. Further studies of elderly insomniacs with midnight awakenings are needed.', u'Status epilepticus (SE) is associated with high mortality in post-anoxic coma despite antiepileptic treatment. The aim of our study was to assess the percentage of awakening in a consecutive series of post-anoxic comatose patients with an EEG pattern consistent with SE and to verify any correlation with clinical, neurophysiological or pharmacological parameters.']\n"
     ]
    }
   ],
   "source": [
    "print data_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abstract_loader(name):\n",
    "    ''' Loads abstracts from json files in provided list\n",
    "    '''\n",
    "    data_list = []\n",
    "    for jsonfile in name:\n",
    "        with codecs.open('benzo_pbabstract/'+jsonfile,\"r\",\"utf-8\") as data_file:  # Note: changed directory of data!\n",
    "            data = json.load(data_file)\n",
    "            values = [replaceNonASCII(value) for value in data.values()]\n",
    "            data_list.extend(values)\n",
    "            \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def splitSentences(abstract):\n",
    "    sentences = sent_tokenize(abstract)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def find_ngrams(sentence, n):\n",
    "    ''' Return list of ngrams from a sentence\n",
    "    '''\n",
    "    words_list = sentence.split()\n",
    "    ngrams = zip(*[words_list[i:] for i in range(n)])\n",
    "    #return [''.join([unicode(w)+' ' for w in ngram if type(w)==unicode]).strip() for ngram in ngrams]\n",
    "    return [''.join([w+' ' for w in ngram]).strip() for ngram in ngrams]\n",
    "\n",
    "\n",
    "#######################\n",
    "# Method 1 - NOT USED #\n",
    "#######################\n",
    "\n",
    "# NOT USED since will return True if finds a food name within a word in the sentence\n",
    "# E.g.: \"pie\" food name and \"therapies\" word in sentence: return True since \"pie\" in \"therapies\"\n",
    "#def includeFoodCmpd(sentence, fdlist):\n",
    "#    if any(word in sentence for word in fdlist):\n",
    "#        return True\n",
    "#    else:\n",
    "#        return False\n",
    "\n",
    "\n",
    "###################################\n",
    "# Method 2 - solution to method 2 #\n",
    "###################################\n",
    "\n",
    "def includeDrug(sentence, dglist, limit, verbose=False):\n",
    "    ''' Calculates the Jaro Wrinkler distance between drug name and ngrams in the sentence.\n",
    "        Returns True if distance > limit\n",
    "    '''\n",
    "    result = False\n",
    "    for drug in dglist:\n",
    "        n = min(5, len(drug.split()))  # Assuming max as 5-gram        \n",
    "        sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                # last word. For now ok, since the JW will still be > limit\n",
    "        for ngram in sentence_ngrams:\n",
    "            # Note: when using jaro_winkler, need to convert into unicode format\n",
    "            ngram_distance = jellyfish.jaro_winkler(u\"{}\".format(drug.lower()), u\"{}\".format(ngram.lower()))\n",
    "            if verbose:\n",
    "                print drug, ngram, ngram_distance\n",
    "            if ngram_distance > limit:  \n",
    "                result = True\n",
    "                break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def includeFoodCmpd(sentence, fdlist, limit, verbose=False):\n",
    "    ''' Calculates the Jaro Wrinkler distance between food name and ngrams in the sentence.\n",
    "        Returns True if distance > 0.95\n",
    "    '''\n",
    "    result = False\n",
    "    for food in fdlist:\n",
    "        n = min(3, len(food.split()))  # Assuming max as trigram        \n",
    "        sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                # last word. For now ok, since the JW will still be > 0.95\n",
    "        for ngram in sentence_ngrams:\n",
    "            # Note: when using jaro_winkler, need to convert into unicode format\n",
    "            ngram_distance = jellyfish.jaro_winkler(food.lower(), u\"{}\".format(ngram.lower()))\n",
    "            if verbose:\n",
    "                print food, ngram, ngram_distance\n",
    "            if ngram_distance >= limit:  \n",
    "                result = True\n",
    "                break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def findFood(sentence, foods):\n",
    "    ''' Returns the food names found in the sentence based on string distance method\n",
    "    '''\n",
    "    \n",
    "    food = []\n",
    "\n",
    "    # If using string distance method:\n",
    "    for f in foods:\n",
    "        n = min(3, len(f.split()))  # Assuming max as trigram        \n",
    "        sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                # last word. For now ok, since the JW will still be > 0.95\n",
    "        for ngram in sentence_ngrams:\n",
    "            # Note: when using jaro_winkler, need to convert into unicode format\n",
    "            if jellyfish.jaro_winkler(f.lower(), u\"{}\".format(ngram.lower())) > 0.95:\n",
    "                food.append(f)\n",
    "            \n",
    "    return food\n",
    "\n",
    "\n",
    "# Not used since we will use the string distance for filtering\n",
    "def findFoodItems(sentence, foods):\n",
    "    for item in foods:\n",
    "        if item in sentence:\n",
    "            print item\n",
    "            \n",
    "def is_relevant(sentence, irrelevant_list, fdlist):\n",
    "    ''' Filter sentences with only relevant food words\n",
    "    '''\n",
    "    foods = findFood(sentence, fdlist)\n",
    "    if any(word in foods for word in irrelevant_list):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'We are working',\n",
       " u'are working hard',\n",
       " u'working hard on',\n",
       " u'hard on 266',\n",
       " u'on 266 project',\n",
       " u'266 project baseline']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test find_ngrams:\n",
    "string = u\"We are working hard on 266 project baseline\"\n",
    "find_ngrams(string, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello Panda 0.0\n",
      "hello is 0.0\n",
      "hello eating 0.455555555556\n",
      "hello a 0.0\n",
      "hello pie 0.511111111111\n",
      "pie Panda 0.511111111111\n",
      "pie is 0.0\n",
      "pie eating 0.5\n",
      "pie a 0.0\n",
      "pie pie 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing includeFoodCmpd function\n",
    "ss = u'Panda is eating a pie'\n",
    "testlist = [u\"hello\",u\"pie\"]\n",
    "includeFoodCmpd(ss, testlist, 0.98, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello Panda 0.0\n",
      "hello is 0.0\n",
      "hello eating 0.455555555556\n",
      "hello a 0.0\n",
      "hello pie 0.511111111111\n",
      "pie Panda 0.511111111111\n",
      "pie is 0.0\n",
      "pie eating 0.5\n",
      "pie a 0.0\n",
      "pie pie 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing includeDrug function\n",
    "ss = u'Panda is eating a pie'\n",
    "testlist = [u\"hello\",u\"pie\"]\n",
    "includeDrug(ss, testlist, 0.99, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all abstracts - there are 744 json files from PubMed\n",
    "filename_list = []\n",
    "for i in xrange(1, 745):\n",
    "    filename_temp = \"pbabstract\" + str(i)+ \".json\"\n",
    "    filename_list.append(filename_temp)\n",
    "\n",
    "abstract = abstract_loader(filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Next-day residual effects are a common problem with current hypnotics. The purpose of the present study was to evaluate the residual effects of eszopiclone on the physical and cognitive functions of healthy elderly people in the early morning and the day following drug administration. Four men and six women aged 63-72unkyears were administered eszopiclone 1unkmg or placebo in a randomized, double-blind and crossover design. Measures of objective parameters and subjective ratings were obtained at 4:00, 6:00, and every 2unkh from 6:00 to 16:00unkhours. For the timed up-and-go test, the main effects of time were seen. For the critical flicker fusion, eszopiclone had significantly worse results compared to placebo in early morning (4:00). There were no significant differences between eszopiclone and placebo in other objective assessments. For the sleep latency, eszopiclone had significantly shorter results compared to placebo (eszopiclone vs placebounk=unk28.4 vs 52.5unkmin, punk=unk0.047). Feeling of deep sleep and the number of wake after sleep onset did not show any significant differences between eszopiclone and placebo. Based on the above results, the changes of physical and cognitive functions in the healthy elderly after taking hypnotics, it was found that eszopiclone 1unkmg is likely to be unharmful for the healthy elderly. Further studies of elderly insomniacs with midnight awakenings are needed.',\n",
       " u'Status epilepticus (SE) is associated with high mortality in post-anoxic coma despite antiepileptic treatment. The aim of our study was to assess the percentage of awakening in a consecutive series of post-anoxic comatose patients with an EEG pattern consistent with SE and to verify any correlation with clinical, neurophysiological or pharmacological parameters.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstractRDD = sc.parallelize(abstract)\\\n",
    "                .filter(lambda a: a is not None)  #Some lines were empty: need to filter out\n",
    "abstractRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: tried to braodcast the foodlist but got an error message when used it in below filter\n",
    "# \"TypeError: 'Broadcast' object is not iterable\".... any idea why?\n",
    "# foodlist_bcast = sc.broadcast(foodlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If substituting drugs related words by \"ACEI\" then filter\n",
    "#drugkeyword = \"ACEI\"\n",
    "\n",
    "#sentences = abstractRDD.flatMap(splitSentences) \\\n",
    "#                       .map(lambda a: pb.ace_substitutor(a, drugkeyword)) \\\n",
    "#                       .filter(lambda a: drugkeyword in a)\\\n",
    "#                       .filter(lambda a: includeFoodCmpd(a, foodlist, 0.98))\n",
    "\n",
    "#%%time\n",
    "#filtered_sentences = sentences.collect()\n",
    "### results:\n",
    "### CPU times: user 71.2 ms, sys: 30.3 ms, total: 101 ms\n",
    "### Wall time: 21min 30s\n",
    "\n",
    "#len(filtered_sentences) -> 938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = abstractRDD.flatMap(splitSentences) \\\n",
    "                       .filter(lambda a: includeDrug(a, drug_options , 0.99))\\\n",
    "                       .filter(lambda a: includeFoodCmpd(a, foodlist, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Conclusions Nuclear division index values, as indicators of cytostaticity and cytotoxicity suggested that investigated inclusion diazepam complexes induced accelerated proliferation of human peripheral blood lymphocytes in vitro, therefore possibly shortening the duration and dynamics of the cell cycle.',\n",
       " u'Stability of benzodiazepines in water was evaluated by soaking an aliquot of hair for up to 30h in deionized water.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 125 ms, sys: 56.2 ms, total: 181 ms\n",
      "Wall time: 41min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filtered_sentences = sentences.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "842"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete some sentences with irrelevant food names\n",
    "irrelevant = ['date', 'rabbit', 'shortening', 'arepas', 'water', 'cocktail']\n",
    "\n",
    "relevant_sentences = sc.parallelize(filtered_sentences)\\\n",
    "                       .filter(lambda a: is_relevant(a, irrelevant, foodlist))\\\n",
    "                       .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/FilteredSentences_Benzo_commonFood_clean.txt\", \"w\") as outcomes:\n",
    "    for sentence in relevant_sentences:\n",
    "        food = findFood(sentence, foodlist)\n",
    "        outcomes.writelines(str(food) + sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Classify sentences as positive or negative based on a sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 467 sentences containing both ACE Inhibitor drug and a food name from Step 2. They will be labeled and then compared to the 2 baseline models classification for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE 1: 50/50 coin flip classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model is to simply classify the interaction between drug and food in the sentence as positive, negative or neutral based on equal probability of 1/3 in each class.  \n",
    "\n",
    "Evaluation metrics: the labels of 100 randomly chosen sentences from outcomes in step 2 (i.e. sentences with drug and food names) will be compared with the random labels from baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classification = [(random.choice([\"positive\", \"negative\", \"neutral\"]), sentence) for sentence in relevant_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------- SECTION BELOW HAS NOT BEEN UPDATED --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE 2: Sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment lexicon used is the Harvard General Inquirer (http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm). It contains 1,915 positive words and 2,291 negative words and is free for research use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (63,108,109,110,176) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "db_sentiment = pd.read_csv(\"data/inquirerbasic.csv\", encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Source</th>\n",
       "      <th>Positiv</th>\n",
       "      <th>Negativ</th>\n",
       "      <th>Pstv</th>\n",
       "      <th>Affil</th>\n",
       "      <th>Ngtv</th>\n",
       "      <th>Hostile</th>\n",
       "      <th>Strong</th>\n",
       "      <th>Power</th>\n",
       "      <th>...</th>\n",
       "      <th>Anomie</th>\n",
       "      <th>NegAff</th>\n",
       "      <th>PosAff</th>\n",
       "      <th>SureLw</th>\n",
       "      <th>If</th>\n",
       "      <th>NotLw</th>\n",
       "      <th>TimeSpc</th>\n",
       "      <th>FormLw</th>\n",
       "      <th>Othtags</th>\n",
       "      <th>Defined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DET ART</td>\n",
       "      <td>| article: Indefinite singular article--some o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ngtv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>H4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noun</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABATE</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABATEMENT</td>\n",
       "      <td>Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entry Source Positiv  Negativ Pstv Affil  Ngtv Hostile Strong Power  \\\n",
       "0            A  H4Lvd     NaN      NaN  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "1      ABANDON  H4Lvd     NaN  Negativ  NaN   NaN  Ngtv     NaN    NaN   NaN   \n",
       "2  ABANDONMENT     H4     NaN  Negativ  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "3        ABATE  H4Lvd     NaN  Negativ  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "4    ABATEMENT    Lvd     NaN      NaN  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "\n",
       "                         ...                         Anomie NegAff PosAff  \\\n",
       "0                        ...                            NaN    NaN    NaN   \n",
       "1                        ...                            NaN    NaN    NaN   \n",
       "2                        ...                            NaN    NaN    NaN   \n",
       "3                        ...                            NaN    NaN    NaN   \n",
       "4                        ...                            NaN    NaN    NaN   \n",
       "\n",
       "  SureLw   If NotLw TimeSpc FormLw  Othtags  \\\n",
       "0    NaN  NaN   NaN     NaN    NaN  DET ART   \n",
       "1    NaN  NaN   NaN     NaN    NaN     SUPV   \n",
       "2    NaN  NaN   NaN     NaN    NaN     Noun   \n",
       "3    NaN  NaN   NaN     NaN    NaN     SUPV   \n",
       "4    NaN  NaN   NaN     NaN    NaN     Noun   \n",
       "\n",
       "                                             Defined  \n",
       "0  | article: Indefinite singular article--some o...  \n",
       "1                                                  |  \n",
       "2                                                  |  \n",
       "3                                                  |  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 186 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry\n",
      "Source\n",
      "Positiv\n",
      "Negativ\n",
      "Pstv\n",
      "Affil\n",
      "Ngtv\n",
      "Hostile\n",
      "Strong\n",
      "Power\n",
      "Weak\n",
      "Submit\n",
      "Active\n",
      "Passive\n",
      "Pleasur\n",
      "Pain\n",
      "Feel\n",
      "Arousal\n",
      "EMOT\n",
      "Virtue\n",
      "Vice\n",
      "Ovrst\n",
      "Undrst\n",
      "Academ\n",
      "Doctrin\n",
      "Econ@\n",
      "Exch\n",
      "ECON\n",
      "Exprsv\n",
      "Legal\n",
      "Milit\n",
      "Polit@\n",
      "POLIT\n",
      "Relig\n",
      "Role\n",
      "COLL\n",
      "Work\n",
      "Ritual\n",
      "SocRel\n",
      "Race\n",
      "Kin@\n",
      "MALE\n",
      "Female\n",
      "Nonadlt\n",
      "HU\n",
      "ANI\n",
      "PLACE\n",
      "Social\n",
      "Region\n",
      "Route\n",
      "Aquatic\n",
      "Land\n",
      "Sky\n",
      "Object\n",
      "Tool\n",
      "Food\n",
      "Vehicle\n",
      "BldgPt\n",
      "ComnObj\n",
      "NatObj\n",
      "BodyPt\n",
      "ComForm\n",
      "COM\n",
      "Say\n",
      "Need\n",
      "Goal\n",
      "Try\n",
      "Means\n",
      "Persist\n",
      "Complet\n",
      "Fail\n",
      "NatrPro\n",
      "Begin\n",
      "Vary\n",
      "Increas\n",
      "Decreas\n",
      "Finish\n",
      "Stay\n",
      "Rise\n",
      "Exert\n",
      "Fetch\n",
      "Travel\n",
      "Fall\n",
      "Think\n",
      "Know\n",
      "Causal\n",
      "Ought\n",
      "Perceiv\n",
      "Compare\n",
      "Eval@\n",
      "EVAL\n",
      "Solve\n",
      "Abs@\n",
      "ABS\n",
      "Quality\n",
      "Quan\n",
      "NUMB\n",
      "ORD\n",
      "CARD\n",
      "FREQ\n",
      "DIST\n",
      "Time@\n",
      "TIME\n",
      "Space\n",
      "POS\n",
      "DIM\n",
      "Rel\n",
      "COLOR\n",
      "Self\n",
      "Our\n",
      "You\n",
      "Name\n",
      "Yes\n",
      "No\n",
      "Negate\n",
      "Intrj\n",
      "IAV\n",
      "DAV\n",
      "SV\n",
      "IPadj\n",
      "IndAdj\n",
      "PowGain\n",
      "PowLoss\n",
      "PowEnds\n",
      "PowAren\n",
      "PowCon\n",
      "PowCoop\n",
      "PowAuPt\n",
      "PowPt\n",
      "PowDoct\n",
      "PowAuth\n",
      "PowOth\n",
      "PowTot\n",
      "RcEthic\n",
      "RcRelig\n",
      "RcGain\n",
      "RcLoss\n",
      "RcEnds\n",
      "RcTot\n",
      "RspGain\n",
      "RspLoss\n",
      "RspOth\n",
      "RspTot\n",
      "AffGain\n",
      "AffLoss\n",
      "AffPt\n",
      "AffOth\n",
      "AffTot\n",
      "WltPt\n",
      "WltTran\n",
      "WltOth\n",
      "WltTot\n",
      "WlbGain\n",
      "WlbLoss\n",
      "WlbPhys\n",
      "WlbPsyc\n",
      "WlbPt\n",
      "WlbTot\n",
      "EnlGain\n",
      "EnlLoss\n",
      "EnlEnds\n",
      "EnlPt\n",
      "EnlOth\n",
      "EnlTot\n",
      "SklAsth\n",
      "SklPt\n",
      "SklOth\n",
      "SklTot\n",
      "TrnGain\n",
      "TrnLoss\n",
      "TranLw\n",
      "MeansLw\n",
      "EndsLw\n",
      "ArenaLw\n",
      "PtLw\n",
      "Nation\n",
      "Anomie\n",
      "NegAff\n",
      "PosAff\n",
      "SureLw\n",
      "If\n",
      "NotLw\n",
      "TimeSpc\n",
      "FormLw\n",
      "Othtags\n",
      "Defined\n"
     ]
    }
   ],
   "source": [
    "# Look at all data fields available\n",
    "for column in db_sentiment.columns:\n",
    "    print column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** some columns seem quite interesting for analysing relationship other than simply positive or negative sentiment (e.g. \"causal\", etc.). For the baseline, we will only use the \"positive\" and \"negative\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter only the words labeled positive or negative\n",
    "positive = db_sentiment[db_sentiment.Positiv == \"Positiv\"].Entry.map(lambda x: x.lower()).tolist()\n",
    "negative = db_sentiment[db_sentiment.Negativ == \"Negativ\"].Entry.map(lambda x: x.lower()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abide', u'ability', u'able', u'abound']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abandon', u'abandonment', u'abate', u'abdicate']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform list into sets for faster search\n",
    "positive = set(positive)\n",
    "negative = set(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an attempt to classify whether a sentence is positive or negative.  \n",
    "Note the main weaknesses:  \n",
    "1. It is \"positively\" biased for now since looks at the positive words first and if it finds it, then it immediately returns positive. Thus, it may not look at the entire sentence in case of both positive or negative words.  \n",
    "2. Negation of a positive word is not taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def includeSentiment(sentence, poslist, neglist): \n",
    "    ''' Classify sentence as positive or negative based on first word found\n",
    "        in the lexicon\n",
    "    '''\n",
    "    if any(word in sentence for word in poslist):\n",
    "        return (\"positive\", sentence)\n",
    "    elif any(word in sentence for word in neglist):\n",
    "        return (\"negative\", sentence)\n",
    "    else:\n",
    "        return (\"neutral\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add sentiment as key in the RDD\n",
    "\n",
    "filtered_sentencesRDD = sc.parallelize(filtered_sentences)\n",
    "sentiments = filtered_sentencesRDD.map(lambda a: includeSentiment(a, positive, negative))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positive',\n",
       "  u'in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.'),\n",
       " ('positive',\n",
       "  u'digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.')]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at 2 first lines\n",
    "sentiments.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 2 lists of filtered sentences: one positive list and one negative list\n",
    "pos_sentiments = sentiments.lookup(\"positive\")\n",
    "neg_sentiments = sentiments.lookup(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.',\n",
       " u'digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.',\n",
       " u'the peptides from whey and caseins exert significant ACEI inhibitory activities comparable to that of captopril, an antihypertensive drug, exhibiting ic50 values of 4.45unkunkm and 4.27unkunkm, respectively.',\n",
       " u'the results introduce, for the first time, new potent ACEI-inhibitory peptides that can be released by gastric pepsin of goat milk whey and caseins and thus may pave the way for their candidacy as anti-hypertensive bioactive peptides and prevention of associated disorders.',\n",
       " u'ACEI use between 1 january 2003 and the index date were determined by the date of hospitalization for acute pancreatitis among the cases.',\n",
       " u'compared to hydrolysates from whey protein, where the inhibitory effect can almost exclusively be attributed to ile-trp, the ACEI inhibition by plant protein hydrolysates is caused by a variety of peptides, in particular tyrosine-containing peptides.',\n",
       " u'the annual rate of adverse events related to ACEI (ie, the number of reported cases of adverse events per 1000 patients receiving an ACEI) was calculated from data captured on the date the events were first reported for the 5 years before and 5 years after the revocation of the pa constraint.',\n",
       " u'these findings suggest a therapeutic potential for mas and/or at2 receptor activation and ACEI inhibition in restoring endothelial function impaired by elevated dietary salt intake or other pathological conditions.',\n",
       " u'casein haplotype significantly influenced the antioxidative and ACEI-inhibitory capacities of digested casein.',\n",
       " u'in particular, bb-a(2)a(1)-aa casein and bb-a(1)a(1)-aa casein showed the highest ACEI-inhibitory capacity, bb-a(2)a(2)-aa casein showed the highest antioxidant capacity, whereas bb-a(2)a(2)-bb casein showed the lowest biological capacity.']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of sentences with positive sentiment lexicon\n",
    "pos_sentiments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the late-eluting fraction (f4) of either whey or caseins exhibited greater ACEI inhibition.',\n",
       " u'we investigated the molecular mechanisms involved in the ACEI (ACEI) inhibition by (-)-epigallocatechin-3-gallate (egcg), a major tea catechin.']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of sentences with negative sentiment lexicon\n",
    "neg_sentiments[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findSentiment(sentence, sentiment, poslist, neglist):\n",
    "    '''Print out the lexicon word that classified the sentence as positive or negative\n",
    "    '''\n",
    "    if sentiment == \"positive\":\n",
    "        lexicon = poslist\n",
    "    else:\n",
    "        lexicon = neglist\n",
    "    for word in lexicon:\n",
    "        # Note: had to use this \"try/except\" since there was an unicode ascii error... any ways\n",
    "        # to fix this without this try/except? if left the same, then we won't be able to see\n",
    "        # some sentiment word in some sentences.\n",
    "        try:\n",
    "            if word in sentence:\n",
    "                print word\n",
    "        except:\n",
    "            next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'casein', u'whey']\n",
      "pro\n",
      "generate\n",
      "significant\n"
     ]
    }
   ],
   "source": [
    "s5 = u\"digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins\"\n",
    "print findFood(s5, foodlist)\n",
    "findSentiment(s5, \"positive\", positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'casein', u'whey']\n",
      "inhibit\n",
      "inhibition\n"
     ]
    }
   ],
   "source": [
    "s6 = u\"the late-eluting fraction (f4) of either whey or caseins exhibited greater ACEI inhibition\"\n",
    "print findFood(s6, foodlist)\n",
    "findSentiment(s6, \"negative\", positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTags(sentence, sentiment, foods, poslist, neglist, limit):\n",
    "    ''' Returns the tags of the sentence \n",
    "        Both lexicon word that classified the sentence as positive or negative and food name\n",
    "    '''\n",
    "    \n",
    "    sent = []\n",
    "    food = []\n",
    "    \n",
    "    if sentiment == \"positive\":\n",
    "        lexicon = poslist\n",
    "    else:\n",
    "        lexicon = neglist\n",
    "    for word in lexicon:\n",
    "        # Note: had to use this \"try/except\" since there was an unicode ascii error... any ways\n",
    "        # to fix this without this try/except? if left the same, then we won't be able to see\n",
    "        # some sentiment word in some sentences.\n",
    "        try:\n",
    "            if word in sentence:\n",
    "                sent.append(word)\n",
    "        except:\n",
    "            next\n",
    "            \n",
    "    # If using simple test of if food name \"in\" sentence method\n",
    "    #for f in foods:\n",
    "    #    if f in sentence:\n",
    "    #        food.append(f)\n",
    "    \n",
    "    # If using string distance method:\n",
    "    for f in foods:\n",
    "        n = min(3, len(f.split()))  # Assuming max as trigram        \n",
    "        sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                # last word. For now ok, since the JW will still be > 0.95\n",
    "        for ngram in sentence_ngrams:\n",
    "            # Note: when using jaro_winkler, need to convert into unicode format\n",
    "            if jellyfish.jaro_winkler(f.lower(), u\"{}\".format(ngram.lower())) > limit:\n",
    "                food.append(f)\n",
    "\n",
    "            \n",
    "    return [sent, food, sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save results in a text file\n",
    "# Note: this could have been also done in Spark! But felt lazy to code... feel free to try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/Positive.txt\", \"w\") as pos:\n",
    "    for sentence in pos_sentiments:\n",
    "        tags = findTags(sentence, \"positive\", foodlist, positive, negative, 0.95)\n",
    "        pos.writelines(str(tags)+ \"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/Negative.txt\", \"w\") as pos:\n",
    "    for sentence in pos_sentiments:\n",
    "        tags = findTags(sentence, \"negative\", foodlist, positive, negative, 0.95)\n",
    "        pos.writelines(str(tags)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes**  \n",
    "1. In our baseline, some words like \"date\" that appear in a sentence will be interpreted as the fruit \"date\" instead of a calendar date and thus, will be filtered as outputs sentences. This can only be solved if we take into account the context of the sentence and we will need ML to model this!  \n",
    "2. Sentiment analysis need a major improvement: only basing on the positive and negative words without how the food and drug are connected through these words is not a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
