{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the Pubmed Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pubmed.utils as pb\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import string\n",
    "# utf-8 support\n",
    "import codecs\n",
    "import nltk\n",
    "# spit abstracts to sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "#pandas!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orig_dict = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "import re \n",
    "#for item in open(\"sent_files/glp_sent.tsv\",\"r\"):\n",
    "for item in open(\"/Users/lisabarcelo/Desktop/W266/food_drug_interaction/parsing/data/labeled_dataAll.tsv\",\"r\").readlines()[1:1000]:\n",
    "    #sent_id, sentiment, compound, sentence = item.split(\"\\t\",3)\n",
    "    i += 1\n",
    "    id_, label, label_num, drug, component, sentence = item.split(\"\\t\",5)\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
    "    sentence = sentence.replace(\";\",\" \")\n",
    "    sentence = sentence.replace('\"',' ')\n",
    "    #sentence = re.sub(\"\\d+\",\"\",re.sub(r'[^\\w\\s]','',sentence))\n",
    "    words_ = [w for w in sentence.split(\" \")]\n",
    "    if len(words_) >= 30:\n",
    "        #print \" \".join([w for w in words_][:30]),\".\"\n",
    "        orig_dict['Sentence #'+str(i)] = label\n",
    "    else:\n",
    "        #print sentence.encode('utf-8')\n",
    "        orig_dict['Sentence #'+str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sent_files/for_SSA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sent_files/for_SSA.py\n",
    "\n",
    "import re \n",
    "for item in open(\"/Users/lisabarcelo/Desktop/W266/food_drug_interaction/parsing/data/labeled_dataAll.tsv\",\"r\").readlines()[1:1000]:\n",
    "    id_, label, label_num, drug, component, sentence = item.split(\"\\t\",5)\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
    "    sentence = sentence.replace(\";\",\" \")\n",
    "    sentence = sentence.replace('\"',' ')\n",
    "    sentence = re.sub(\"\\d+\",\"\",re.sub(r'[^\\w\\s]','',sentence))\n",
    "    words_ = [w for w in sentence.split(\" \")]\n",
    "    if len(words_) >= 30:\n",
    "        print \" \".join([w for w in words_][:30]),\".\"\n",
    "    else:\n",
    "        print sentence.encode('utf-8'),\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python2 sent_files/for_SSA.py > sent_files/ssa_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iodine labelled betamethyliodophenyl pentadecanoic acid myocardial single photon emission computed tomography showed decreased uptake of the inferior segment in the early image whereas the delayed images revealed significant fillin .\r\n",
      "Phenylethanol is a widely used aroma compound with roselike fragrance and Lhomophenylalanine is a building block of angiotensinconverting enzyme  inhibitor\r\n",
      ".\r\n",
      " dihydroxyphenylacetic acid and methylcatechol  also inhibited both APN and ACE activity but not NEP activity   \r\n",
      ".\r\n",
      "A well microplate format of this method was used to screen the ACE inhibitory potential of peptides fractionated from an enzymatic hydrolysate of arachin\r\n",
      ".\r\n",
      " A biochemical study was performed in order to analyse the effect of the NOdonors SIN and diethylamineNO  and of an aqueous solution of nitric oxide on the ACE .\r\n",
      "A combination of ACEI  and VK  was administered for  months following curative therapy for HCC\r\n",
      ".\r\n"
     ]
    }
   ],
   "source": [
    "!head sent_files/ssa_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x lexparser.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Line Sentiment Analysis\n",
    "\n",
    "This creates an output file with tuples and sentiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2.5 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.6 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [1.1 sec].\n",
      "[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [3.0 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator dcoref\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sentiment\n",
      "\n",
      "Processing file /Users/lisabarcelo/Desktop/W266/food_drug_interaction/parsing/sent_files/ssa_all.txt ... writing to /Users/lisabarcelo/Desktop/W266/food_drug_interaction/parsing/ssa_all.txt.out\n",
      "Annotating file /Users/lisabarcelo/Desktop/W266/food_drug_interaction/parsing/sent_files/ssa_all.txt ... done [562.5 sec].\n",
      "\n",
      "Annotation pipeline timing information:\n",
      "TokenizerAnnotator: 0.4 sec.\n",
      "WordsToSentencesAnnotator: 0.1 sec.\n",
      "POSTaggerAnnotator: 5.4 sec.\n",
      "MorphaAnnotator: 0.2 sec.\n",
      "NERCombinerAnnotator: 19.8 sec.\n",
      "ParserAnnotator: 134.5 sec.\n",
      "DeterministicCorefAnnotator: 399.0 sec.\n",
      "SentimentAnnotator: 3.3 sec.\n",
      "TOTAL: 562.5 sec. for 24540 tokens at 43.6 tokens/sec.\n",
      "Pipeline setup: 14.4 sec.\n",
      "Total time for StanfordCoreNLP pipeline: 578.3 sec.\n"
     ]
    }
   ],
   "source": [
    "!java -cp \"*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP \\\n",
    "-annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref,sentiment \\\n",
    "-file sent_files/ssa_all.txt \\\n",
    "-outputFormat text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat ssa_all.txt.out > sent_files/output_file.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #1 (30 tokens, sentiment: Negative):\r\n",
      "iodine labelled betamethyliodophenyl pentadecanoic acid myocardial single photon emission computed tomography showed decreased uptake of the inferior segment in the early image whereas the delayed images revealed significant fillin .\r\n",
      "[Text=iodine CharacterOffsetBegin=1 CharacterOffsetEnd=7 PartOfSpeech=NN Lemma=iodine NamedEntityTag=O SentimentClass=Neutral]\r\n",
      "[Text=labelled CharacterOffsetBegin=8 CharacterOffsetEnd=16 PartOfSpeech=VBN Lemma=label NamedEntityTag=O SentimentClass=Neutral]\r\n",
      "[Text=betamethyliodophenyl CharacterOffsetBegin=17 CharacterOffsetEnd=37 PartOfSpeech=NN Lemma=betamethyliodophenyl NamedEntityTag=O SentimentClass=Neutral]\r\n",
      "[Text=pentadecanoic CharacterOffsetBegin=38 CharacterOffsetEnd=51 PartOfSpeech=JJ Lemma=pentadecanoic NamedEntityTag=O SentimentClass=Neutral]\r\n",
      "[Text=acid CharacterOffsetBegin=52 CharacterOffsetEnd=56 PartOfSpeech=NN Lemma=acid NamedEntityTag=O SentimentClass=Neutral]\r\n",
      "[Text=myocardial CharacterOffsetBegin=57 CharacterOffsetEnd=67 PartOfSpeech=JJ Lemma=myocardial NamedEntityTag=O SentimentClass=Neutral]\r\n",
      "[Text=single CharacterOffsetBegin=68 CharacterOffsetEnd=74 PartOfSpeech=JJ Lemma=single NamedEntityTag=O SentimentClass=Neutral]\r\n",
      "[Text=photon CharacterOffsetBegin=75 CharacterOffsetEnd=81 PartOfSpeech=NN Lemma=photon NamedEntityTag=O SentimentClass=Neutral]\r\n"
     ]
    }
   ],
   "source": [
    "!head sent_files/output_file.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "Look up sentences and see how their manual sentiment compares with the SSA sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssa_dict = {}\n",
    "for line in open('sent_files/output_file.json').readlines():\n",
    "    if 'Sentence #' in line:\n",
    "        sent, num, _,_,_,_ = line.split(\" \", 5)\n",
    "        sentiment = str(line.strip('\\n')).split(\":\")[1]\n",
    "        sentiment = re.sub(\"\\d+\",\"\",re.sub(r'[^\\w\\s]','',sentiment))\n",
    "        #print sent,num, sentiment.lower()\n",
    "        ssa_dict[sent + \" \" + num] = sentiment.lower()\n",
    "        \n",
    "        #this is because the SSA also has \"very negative\" as a sentiment\n",
    "        if 'negative' in sentiment:\n",
    "            ssa_dict[sent + \" \" + num] = 'negative'\n",
    "        else:\n",
    "            ssa_dict[sent + \" \" + num] = sentiment.lower().strip(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 vs 999\n"
     ]
    }
   ],
   "source": [
    "print len(orig_dict.keys()), \"vs\", len(ssa_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  13.2 % , Correct:  132 Incorrect:  867  Total:  999\n"
     ]
    }
   ],
   "source": [
    "#Make two Dictionaries\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "total = 0\n",
    "\n",
    "#now iterate through and verify!\n",
    "for k,v in ssa_dict.iteritems():\n",
    "    try:\n",
    "        total += 1\n",
    "        if ssa_dict[k] == orig_dict[k]:\n",
    "            correct +=1\n",
    "        else:\n",
    "            incorrect +=1\n",
    "    except:\n",
    "        pass\n",
    "print \"Accuracy: \", round(float(correct)/float(total),3) * 100, \"% , Correct: \", correct, \"Incorrect: \", incorrect, \" Total: \", total"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
