{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pubmed Embeddings with Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n",
    "import numpy as np\n",
    "import glob, re, os, json, sys, codecs, pickle\n",
    "import nltk\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "#from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras.preprocessing.text\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ Word processing functions -------------------------- #\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"<unk>\" # unknown token\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "def flat_map(sentences):\n",
    "    return [sent for s in sentences for sent in s]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------ convenience functions ----------------------- #\n",
    "\n",
    "def abstract_loader(name):\n",
    "    with codecs.open(name,\"r\",\"utf-8\") as data_file:\n",
    "        data = json.load(data_file)        \n",
    "    return data.values()\n",
    "\n",
    "def get_parent_dir(directory):\n",
    "    import os\n",
    "    return os.path.dirname(directory)\n",
    "\n",
    "current_dirs_parent = get_parent_dir(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "\n",
    "Here we load 220 Open Access articles from the British Journal of Pharmacology as an example\n",
    "\n",
    "### 1. Read in data\n",
    "\n",
    "First, load the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract part: 1229107 / 1229107"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path = '/home/adam/Documents/MIDS/W266/Project/parsing/pbabstract_total*'\n",
    "#path = '/media/adam/Data/PMC/Br_J_Pharmacol/*.txt'\n",
    "# \n",
    "files = glob.glob(path)\n",
    "text = []\n",
    "sentences = []\n",
    "# iterate over the list to get each file \n",
    "for fle in files:\n",
    "    # open the file and then call .read() to get the text \n",
    "    for t in abstract_loader(fle):\n",
    "        text.append(t)\n",
    "# \n",
    "N_TEXT = len(text)\n",
    "for i, t in enumerate(text):\n",
    "    print('\\rAbstract part:', i+1, '/', N_TEXT,end='')\n",
    "    sys.stdout.flush()\n",
    "    try:\n",
    "        sentences.append(nltk.sent_tokenize(t))\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "# build a flat list of sentences\n",
    "sents = flat_map(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6422569"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(sents, open('pubmed_sentences.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then add the compounds from the labelled sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common compounds: [('l-dopa', 16), ('styrene', 14), ('pectin', 14), ('glycyrrhizin', 13), ('sodium nitrite', 12), ('lard', 12), ('procyanidin', 12), ('vinegar', 12), ('genistein', 11), ('pleurotus ostreatus', 11)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "886"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "compounds = Counter()\n",
    "\n",
    "with open(current_dirs_parent + '/data/labelled_sents.csv', 'r') as labelled_sents: \n",
    "    for num, line in enumerate(labelled_sents):\n",
    "        label, compound, sent = line.strip().split('\\t')\n",
    "        compounds[compound] += 1\n",
    "        \n",
    "print('Most common compounds: {}'.format(compounds.most_common(10)))\n",
    "len(compounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497272"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use if Python 2.7\n",
    "#def text_to_word_sequence(text,\n",
    "#                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "#                          lower=True, split=\" \"):\n",
    "#    if lower: text = text.lower()\n",
    "#    try :\n",
    "#        text = unicode(text, \"utf-8\")\n",
    "#    except TypeError:\n",
    "#        pass\n",
    "#    translate_table = {ord(c): ord(t) for c,t in  zip(filters, split*len(filters)) }\n",
    "#    text = text.translate(translate_table)\n",
    "#    seq = text.split(split)\n",
    "#    return [i for i in seq if i]\n",
    "    \n",
    "#keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence\n",
    "\n",
    "# build corpus\n",
    "corpus = [sentence for sentence in sents if sentence.count(' ') >= 2]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "# vocabulary size\n",
    "V = len(tokenizer.word_index) + 1\n",
    "V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Subset vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordset = []\n",
    "for k, v in tokenizer.word_index.items():\n",
    "    if v < 20000:\n",
    "        wordset.append(canonicalize_word(k, digits=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add compounds to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for compound in compounds:\n",
    "    wordset.append(compound)\n",
    "\n",
    "wordset = set(wordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although we set 20000 above as vocabulary size, it becomes a bit smaller because of canonicalization..this could be improved but maybe not a serious problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20269"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_canon = [' '.join(canonicalize_words(sentence.split(), wordset=wordset, digits=False)) for sentence in sents if sentence.count(' ') >= 2]\n",
    "tokenizer_canon = Tokenizer()\n",
    "\n",
    "tokenizer_canon.fit_on_texts(corpus_canon)\n",
    "V_canon = len(tokenizer_canon.word_index) + 1\n",
    "V_canon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define model to train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1, 150)        3040350     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1, 150)        3040350     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1, 1)          0           embedding_1[0][0]                \n",
      "                                                                   embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 1)             0           dot_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1)             0           reshape_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,080,700\n",
      "Trainable params: 6,080,700\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "V = V_canon\n",
    "\n",
    "dim_embedddings = 150\n",
    "\n",
    "# inputs\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "w = Embedding(V, dim_embedddings)(w_inputs)\n",
    "\n",
    "# context\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "c  = Embedding(V, dim_embedddings)(c_inputs)\n",
    "o = Dot(axes=2)([w, c])\n",
    "o = Reshape((1,), input_shape=(1, 1))(o)\n",
    "o = Activation('sigmoid')(o)\n",
    "\n",
    "SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\n",
    "SkipGram.summary()\n",
    "SkipGram.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    loss = 0.\n",
    "    for i, doc in enumerate(tokenizer_canon.texts_to_sequences(corpus_canon)):\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=5, negative_samples=5.)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        if x:\n",
    "            loss += SkipGram.train_on_batch(x, y)\n",
    "\n",
    "    print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f = open('vectors_BrJP.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim_embedddings))\n",
    "vectors = SkipGram.get_weights()[0]\n",
    "for word, i in tokenizer_canon.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9738 100\n",
      "being 0.0593141 0.452268 -0.390305 0.107675 0.00720654 0.135836 0.0530281 0.4407 \b11 -0.0564856 0.130661 -0.458944 0.0229433 0.337225 0.136104 0.172514 0.336811 0 \b.164075 -0.05117 0.25103 -0.114255 0.0270379 -0.0600328 0.0808357 0.0611498 0.01 \b86546 0.088466 0.213662 0.07562 0.194044 -0.17165 -0.0736555 0.231973 -0.246112  \b-0.0715608 0.245144 -0.137682 0.127778 -0.128208 0.374055 0.0761042 0.0242863 -0 \b.187424 0.229611 0.0786641 0.128452 -0.0158993 -0.118428 -0.394745 0.012744 0.25 \b0862 -0.196155 0.0120631 -0.354271 -0.0190368 0.304903 -0.107094 -0.204871 0.057 \b992 0.064876 0.270369 -0.0747011 -0.174183 0.0653418 -0.283184 -0.0255197 -0.215 \b13 0.0202362 0.528384 -0.108962 0.113981 -0.125209 -0.117595 -0.0364177 0.097629 \b2 0.0885717 0.114088 -0.115811 -0.215144 0.547377 0.0633373 -0.241964 0.106268 - \b0.158015 -0.1879 -0.0697495 0.149984 -0.0594108 -0.0208565 0.116233 0.194813 0.1 \b7322 0.341415 0.303702 -0.178648 -0.176093 -0.00402338 0.155215 0.00123318 -0.00 \b311036 0.0270616\n",
      "\u001b[K0.0704968 0.16928 -0.0374137 -0.0334076 0.159611 -0.128862 0.109209 0.0793969 0. \b:\u001b[K"
     ]
    }
   ],
   "source": [
    "!less vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
