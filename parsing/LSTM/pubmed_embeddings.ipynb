{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pubmed Embeddings with Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "#from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras.preprocessing.text\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ Word processing functions -------------------------- #\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"<unk>\" # unknown token\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "def flat_map(sentences):\n",
    "    return [sent for s in sentences for sent in s]\n",
    "\n",
    "current_dirs_parent = get_parent_dir(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "\n",
    "Here we load 220 Open Access articles from the British Journal of Pharmacology as an example\n",
    "\n",
    "### 1. Read in data\n",
    "\n",
    "First, load the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "path = '/media/adam/Data/PMC/Br_J_Pharmacol/*.txt'\n",
    "# \n",
    "files = glob.glob(path)\n",
    "text = []\n",
    "sentences = []\n",
    "# iterate over the list to get each file \n",
    "for fle in files:\n",
    "    # open the file and then call .read() to get the text \n",
    "    with codecs.open(fle, 'rb', encoding='utf-8') as f:\n",
    "        text.append(f.read())\n",
    "# \n",
    "for t in text:\n",
    "    sentences.append(nltk.sent_tokenize(t))\n",
    "\n",
    "# build a flat list of sentences\n",
    "sents = flat_map(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then add the compounds from the labelled sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "compounds = Counter()\n",
    "\n",
    "with open(current_dirs_parent + '/data/labelled_sents.csv', 'r') as labelled_sents: \n",
    "    for num, line in enumerate(labelled_sents):\n",
    "        label, compound, sent = line.strip().split('\\t')\n",
    "        compounds[compound] += 1\n",
    "        \n",
    "print('Most common compounds: {}'.format(compounds.most_common(10)))\n",
    "len(compounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118810"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use if Python 2.7\n",
    "#def text_to_word_sequence(text,\n",
    "#                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "#                          lower=True, split=\" \"):\n",
    "#    if lower: text = text.lower()\n",
    "#    try :\n",
    "#        text = unicode(text, \"utf-8\")\n",
    "#    except TypeError:\n",
    "#        pass\n",
    "#    translate_table = {ord(c): ord(t) for c,t in  zip(filters, split*len(filters)) }\n",
    "#    text = text.translate(translate_table)\n",
    "#    seq = text.split(split)\n",
    "#    return [i for i in seq if i]\n",
    "    \n",
    "#keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence\n",
    "\n",
    "# build corpus\n",
    "corpus = [sentence for sentence in sents if sentence.count(' ') >= 2]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "# vocabulary size\n",
    "V = len(tokenizer.word_index) + 1\n",
    "V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Subset vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordset = []\n",
    "for k, v in tokenizer.word_index.items():\n",
    "    if v < 10000:\n",
    "        wordset.append(canonicalize_word(k, digits=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although we set 10000 above as vocabulary size, it becomes a bit smaller because of canonicalization..this could be improved but maybe not a serious problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_canon = [' '.join(canonicalize_words(sentence.split(), wordset=wordset, digits=False)) for sentence in sents if sentence.count(' ') >= 2]\n",
    "tokenizer_canon = Tokenizer()\n",
    "\n",
    "tokenizer_canon.fit_on_texts(corpus_canon)\n",
    "V_canon = len(tokenizer_canon.word_index) + 1\n",
    "V_canon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: add compounds to wordset before training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define model to train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1, 100)        960000      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1, 100)        960000      input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1, 1)          0           embedding_1[0][0]                \n",
      "                                                                   embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 1)             0           dot_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1)             0           reshape_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,920,000\n",
      "Trainable params: 1,920,000\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "V = V_canon\n",
    "\n",
    "dim_embedddings = 100\n",
    "\n",
    "# inputs\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "w = Embedding(V, dim_embedddings)(w_inputs)\n",
    "\n",
    "# context\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "c  = Embedding(V, dim_embedddings)(c_inputs)\n",
    "o = Dot(axes=2)([w, c])\n",
    "o = Reshape((1,), input_shape=(1, 1))(o)\n",
    "o = Activation('sigmoid')(o)\n",
    "\n",
    "SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\n",
    "SkipGram.summary()\n",
    "SkipGram.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14921.2482462\n",
      "14011.7568922\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    loss = 0.\n",
    "    for i, doc in enumerate(tokenizer_canon.texts_to_sequences(corpus_canon)):\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=5, negative_samples=5.)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        if x:\n",
    "            loss += SkipGram.train_on_batch(x, y)\n",
    "\n",
    "    print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim_embedddings))\n",
    "vectors = SkipGram.get_weights()[0]\n",
    "for word, i in tokenizer_canon.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3919 128\n",
      "limited -0.196793 0.083001 0.0139577 0.0825418 0.175225 -0.0393457 -0.0117669 -0 \b.0876988 -0.113816 0.146876 -0.159492 -0.0642036 0.0866598 0.104659 -0.0438206 - \b0.00766364 0.0989949 -0.171952 -0.0387336 0.12589 -0.10796 0.0164086 0.117531 -0 \b.0330524 -0.130886 0.0488566 0.141625 0.0365962 0.0293824 0.0795345 0.00664139 0 \b.0416241 0.0827253 -0.0701966 -0.0899939 -0.0404796 -0.0572519 -0.100596 -0.0770 \b045 -0.11439 0.107152 -0.0600173 0.0407845 -0.0100802 0.0469915 0.116893 0.05898 \b3 -0.172963 0.00372644 0.220961 -0.0621724 0.00068234 0.0179163 -0.0652906 0.021 \b1658 -0.11143 -0.0318353 -0.118958 0.0968845 0.0745354 0.0688471 0.054137 0.1385 \b28 -0.0278073 0.101461 -0.0322092 -0.0041395 -0.0837396 0.0898334 0.0472739 -0.0 \b333762 0.037255 -0.095782 -0.0916879 -0.0380139 -0.0503423 0.0708811 0.139838 -0 \b.114322 -0.0624948 -0.0718154 0.0730689 0.0822579 0.0474888 -0.116145 0.0916795  \b0.0237692 -0.0564619 -0.116132 -0.0161411 -0.0160286 -0.0950811 -0.0837354 0.081 \b5025 -0.0187882 0.12813 -0.137645 -0.0175226 0.000638438 0.0241309 0.00862714 0. \b185676 0.0147382 -0.116709 0.0808526 0.0818361 -0.0794979 0.0506362 0.0433839 -0 \b.104826 0.0236673 0.192375 0.0390948 -0.229093 0.0913938 0.00798779 0.0442884 0. \b00139162 0.00693747 0.0969691 0.139436 0.0217537 0.13026 -0.187801 0.0161248 -0. \b0625062 0.0622731 -0.09262\n",
      "\u001b[K2806 -0.0797495 -0.0218789 0.0476062 -0.120504 0.0492472 0.0830006 0.0520619 0.0 \b:\u001b[K"
     ]
    }
   ],
   "source": [
    "!less vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
