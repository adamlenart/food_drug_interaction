{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline : Food-drug Interaction Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load food names or compounds into a list of unique items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 (not used): directly load csv data in Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_food = pd.read_csv(\"data/contents copy.csv\", encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>orig_food_common_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Kiwi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Cashew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id orig_food_common_name\n",
       "0   1                  Kiwi\n",
       "1   2                 Onion\n",
       "2   3                 Onion\n",
       "3   4                Chives\n",
       "4   5                Cashew"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_food.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food = db_food[\"orig_food_common_name\"].tolist()  # somehow, converting to a set instead of list didn't work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodlist = set()\n",
    "\n",
    "for f in food:\n",
    "    for term in str(f).lower().split(\",\"):\n",
    "        if len(term) >=3:  # in case some single letter or determinant is included?\n",
    "            foodlist.add(term.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart balance light buttery spread\n",
      "yellow\n",
      "baking chocolate\n",
      "swiss chard stems [red]\n",
      "low salt (includes oyster\n",
      "dry mix\n",
      "diet strawberry kiwi\n",
      "chocolate sandwich\n",
      "endive [escarole]\n",
      "no cholesterol\n",
      "ready -to-heat\n",
      "pancakes plain\n",
      "standard-type\n",
      "immature seeds\n",
      "chocolate cake\n",
      "taco with chicken\n",
      "regular (10 minute)\n",
      "pepeao\n",
      "broiler\n",
      "wholemeal\n",
      "broiled\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for f in foodlist:\n",
    "    print f\n",
    "    i += 1\n",
    "    if i >20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6143"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(foodlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"foodlist.txt\", \"w\", \"utf-8\") as fdlist:\n",
    "    for item in foodlist:\n",
    "        fdlist.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using Adam's pickle file (food common name as sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pickling the data from foodb.ca database, see Adam's notebook **compound_food_id.ipynb**  \n",
    "\n",
    "As a first test, we will use only the food common name (not scientific name) only. Compounds names will be added once this test passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# food_common.pickle: Dictionary with common English food names as keys, compounds as values\n",
    "test = pickle.load(open( \"data/food_common.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food name\n",
      "----------\n",
      "Oregon yampah\n",
      "Okra\n",
      "Black mulberry\n",
      "Avocado\n",
      "Parsley\n",
      "Elderberry\n",
      "Sugar\n",
      "Sweet bay\n",
      "Common bean\n",
      "Fig\n",
      "Lard\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(test.iteritems()):\n",
    "    if i == 0:\n",
    "        print 'food name'\n",
    "        print '-'*10\n",
    "    print '{0}'.format(item[0])\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foodlist_test = set(test.keys())  # Will work with a set rather than a list. Faster search for later (hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"foodlist.txt\", \"w\") as fdlist:\n",
    "    for item in foodlist_test:\n",
    "        fdlist.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: there are 2 words with unusual symbols (dragee, cupuacu) and have been modified in the file and renamed\n",
    "# as \"foodlisr2.txt\"\n",
    "# Any ideas how to deal with this without changing the file manually?? Looks like codecs does not work on pickle file\n",
    "\n",
    "foodlist = set()\n",
    "\n",
    "with open(\"foodlist2.txt\", \"r\") as fdlist2:\n",
    "    for line in fdlist2:\n",
    "        foodlist.add(str(line).lower().strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Filter sentences from abstract with drug keyword and food names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Test with only the abstracts' first json file. Once works, we can add all 100 remaining files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# UTF-8 support\n",
    "import codecs\n",
    "\n",
    "name = \"pbabstract1.json\"\n",
    "with codecs.open(name,\"r\",\"utf-8\") as data_file:\n",
    "    data = json.load(data_file)\n",
    "abstractRDD = sc.parallelize(data.values())  # To load only the values and not the key (ID number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Heart failure still has a significant disease burden with poor outcomes worldwide despite advances in therapy. The standard therapies have been focused on blockade of renin-angiotensin-aldosterone system with angiotensin-converting enzyme inhibitors, angiotensin receptor blockers and mineralocorticoid antagonists and the sympathetic nervous system with \\u03b2-blockers. The natriuretic peptide system is a potential counter-regulatory system that promotes vasodilatation and natriuresis. Angiotensin receptor neprilysin inhibitors are a new class drug capable of blocking the renin-angiotensin-aldosterone system and enhancing the natriuretic peptide system to improve neurohormonal balance. The success of the PARADIGM-HF trial with LCZ696 and its approval for heart failure treatment is likely to generate a paradigm shift. This review summarises the current knowledge of LCZ696 with a focus on pharmacology, pharmacokinetics and pharmacodynamics, mechanisms of action, clinical efficacy and safety.',\n",
       " u'All patients with lower extremity peripheral arterial disease (LE-PAD) should benefit from recommended pharmacologic therapies including antiplatelet agents, angiotensin-converting enzyme (ACE) inhibitors, or angiotensin receptor blockers (ARBs), and hydroxy-methyl-glutaryl-coenzyme A reductase inhibitors (statins). In the present study, this triple therapy was defined as the best medical treatment. This study was designed to determine the number of patients who received best medical treatment at admission and at discharge from a vascular surgery department. We also examined the number of patients who received adapted medical treatment and every pharmacologic class separately. Finally, we investigated whether there were differences in prescribing rates according to patient characteristics and cardiovascular history, clinical grade of LE-PAD, and the type of surgery practiced.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstractRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To use PubMed API\n",
    "import pubmed.utils as pb\n",
    "\n",
    "# Split abstracts to sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "\n",
    "def splitSentences(abstract):\n",
    "    sentences = sent_tokenize(abstract)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drugkeyword = \"ACEI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are working',\n",
       " 'are working hard',\n",
       " 'working hard on',\n",
       " 'hard on 266',\n",
       " 'on 266 project',\n",
       " '266 project baseline']"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_ngrams(sentence, n):\n",
    "    ''' Return list of ngrams from a sentence\n",
    "    '''\n",
    "    words_list = sentence.split()\n",
    "    ngrams = zip(*[words_list[i:] for i in range(n)])\n",
    "    return [''.join([str(w)+' ' for w in ngram if type(w)==str]).strip() for ngram in ngrams]\n",
    "\n",
    "    \n",
    "\n",
    "#example:\n",
    "string = \"We are working hard on 266 project baseline\"\n",
    "find_ngrams(string, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "\n",
    "# Method 1 - NOT USED\n",
    "# NOT USED since will return True if finds a food name within a word in the sentence\n",
    "# E.g.: \"pie\" food name and \"therapies\" word in sentence: return True since \"pie\" in \"therapies\"\n",
    "#def includeFoodCmpd(sentence, fdlist):\n",
    "#    if any(word in sentence for word in fdlist):\n",
    "#        return True\n",
    "#    else:\n",
    "#        return False\n",
    "\n",
    "# Method 2 - solution to method 2\n",
    "def includeFoodCmpd(sentence, fdlist):\n",
    "    ''' Calculates the Jaro Wrinkler distance between food name and ngrams in the sentence.\n",
    "        Returns True if distance > 0.95\n",
    "    '''\n",
    "    result = False\n",
    "    for food in fdlist:\n",
    "        n = min(3, len(food.split()))  # Assuming max as trigram        \n",
    "        try:\n",
    "            sentence = sentence.encode(\"utf-8\")\n",
    "            sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                    # last word. For now ok, since the JW will still be > 0.95\n",
    "            for ngram in sentence_ngrams:\n",
    "                # Note: when using jaro_winkler, need to convert into unicode format\n",
    "                print food, ngram, jellyfish.jaro_winkler(u\"{}\".format(food.lower()), u\"{}\".format(ngram.lower()))\n",
    "                if jellyfish.jaro_winkler(u\"{}\".format(food.lower()), u\"{}\".format(ngram.lower())) > 0.95:  \n",
    "                    result = True\n",
    "                    break\n",
    "        except:\n",
    "            next\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello Panda 0.0\n",
      "hello is 0.0\n",
      "hello eating 0.455555555556\n",
      "hello a 0.0\n",
      "hello pie 0.511111111111\n",
      "pie Panda 0.511111111111\n",
      "pie is 0.0\n",
      "pie eating 0.5\n",
      "pie a 0.0\n",
      "pie pie 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing includeFoodCmpd function\n",
    "ss = u'Panda is eating a pie'\n",
    "testlist = [\"hello\",\"pie\"]\n",
    "includeFoodCmpd(ss, testlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: tried to braodcast the foodlist but got an error message when used it in below filter\n",
    "# \"TypeError: 'Broadcast' object is not iterable\".... any idea why?\n",
    "# foodlist_bcast = sc.broadcast(foodlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = abstractRDD.flatMap(splitSentences) \\\n",
    "                       .map(lambda a: pb.ace_substitutor(a, drugkeyword)) \\\n",
    "                       .filter(lambda a: drugkeyword in a)\\\n",
    "                       .filter(lambda a: includeFoodCmpd(a, foodlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.',\n",
       " u'digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.']"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFoodItems(sentence, foods):\n",
    "    for item in foods:\n",
    "        if item in sentence:\n",
    "            print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "oat\n",
      "whey\n"
     ]
    }
   ],
   "source": [
    "findFoodItems(s, foodlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "oat\n",
      "whey\n"
     ]
    }
   ],
   "source": [
    "s2 = \"digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.\"\n",
    "findFoodItems(s2, foodlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Classify sentences as positive or negative based on a sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment lexicon used is the Harvard General Inquirer (http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm). It contains 1,915 positive words and 2,291 negative words and is free for research use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (63,108,109,110,176) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "db_sentiment = pd.read_csv(\"data/inquirerbasic.csv\", encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Source</th>\n",
       "      <th>Positiv</th>\n",
       "      <th>Negativ</th>\n",
       "      <th>Pstv</th>\n",
       "      <th>Affil</th>\n",
       "      <th>Ngtv</th>\n",
       "      <th>Hostile</th>\n",
       "      <th>Strong</th>\n",
       "      <th>Power</th>\n",
       "      <th>...</th>\n",
       "      <th>Anomie</th>\n",
       "      <th>NegAff</th>\n",
       "      <th>PosAff</th>\n",
       "      <th>SureLw</th>\n",
       "      <th>If</th>\n",
       "      <th>NotLw</th>\n",
       "      <th>TimeSpc</th>\n",
       "      <th>FormLw</th>\n",
       "      <th>Othtags</th>\n",
       "      <th>Defined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DET ART</td>\n",
       "      <td>| article: Indefinite singular article--some o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ngtv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>H4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noun</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABATE</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABATEMENT</td>\n",
       "      <td>Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entry Source Positiv  Negativ Pstv Affil  Ngtv Hostile Strong Power  \\\n",
       "0            A  H4Lvd     NaN      NaN  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "1      ABANDON  H4Lvd     NaN  Negativ  NaN   NaN  Ngtv     NaN    NaN   NaN   \n",
       "2  ABANDONMENT     H4     NaN  Negativ  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "3        ABATE  H4Lvd     NaN  Negativ  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "4    ABATEMENT    Lvd     NaN      NaN  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "\n",
       "                         ...                         Anomie NegAff PosAff  \\\n",
       "0                        ...                            NaN    NaN    NaN   \n",
       "1                        ...                            NaN    NaN    NaN   \n",
       "2                        ...                            NaN    NaN    NaN   \n",
       "3                        ...                            NaN    NaN    NaN   \n",
       "4                        ...                            NaN    NaN    NaN   \n",
       "\n",
       "  SureLw   If NotLw TimeSpc FormLw  Othtags  \\\n",
       "0    NaN  NaN   NaN     NaN    NaN  DET ART   \n",
       "1    NaN  NaN   NaN     NaN    NaN     SUPV   \n",
       "2    NaN  NaN   NaN     NaN    NaN     Noun   \n",
       "3    NaN  NaN   NaN     NaN    NaN     SUPV   \n",
       "4    NaN  NaN   NaN     NaN    NaN     Noun   \n",
       "\n",
       "                                             Defined  \n",
       "0  | article: Indefinite singular article--some o...  \n",
       "1                                                  |  \n",
       "2                                                  |  \n",
       "3                                                  |  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 186 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry\n",
      "Source\n",
      "Positiv\n",
      "Negativ\n",
      "Pstv\n",
      "Affil\n",
      "Ngtv\n",
      "Hostile\n",
      "Strong\n",
      "Power\n",
      "Weak\n",
      "Submit\n",
      "Active\n",
      "Passive\n",
      "Pleasur\n",
      "Pain\n",
      "Feel\n",
      "Arousal\n",
      "EMOT\n",
      "Virtue\n",
      "Vice\n",
      "Ovrst\n",
      "Undrst\n",
      "Academ\n",
      "Doctrin\n",
      "Econ@\n",
      "Exch\n",
      "ECON\n",
      "Exprsv\n",
      "Legal\n",
      "Milit\n",
      "Polit@\n",
      "POLIT\n",
      "Relig\n",
      "Role\n",
      "COLL\n",
      "Work\n",
      "Ritual\n",
      "SocRel\n",
      "Race\n",
      "Kin@\n",
      "MALE\n",
      "Female\n",
      "Nonadlt\n",
      "HU\n",
      "ANI\n",
      "PLACE\n",
      "Social\n",
      "Region\n",
      "Route\n",
      "Aquatic\n",
      "Land\n",
      "Sky\n",
      "Object\n",
      "Tool\n",
      "Food\n",
      "Vehicle\n",
      "BldgPt\n",
      "ComnObj\n",
      "NatObj\n",
      "BodyPt\n",
      "ComForm\n",
      "COM\n",
      "Say\n",
      "Need\n",
      "Goal\n",
      "Try\n",
      "Means\n",
      "Persist\n",
      "Complet\n",
      "Fail\n",
      "NatrPro\n",
      "Begin\n",
      "Vary\n",
      "Increas\n",
      "Decreas\n",
      "Finish\n",
      "Stay\n",
      "Rise\n",
      "Exert\n",
      "Fetch\n",
      "Travel\n",
      "Fall\n",
      "Think\n",
      "Know\n",
      "Causal\n",
      "Ought\n",
      "Perceiv\n",
      "Compare\n",
      "Eval@\n",
      "EVAL\n",
      "Solve\n",
      "Abs@\n",
      "ABS\n",
      "Quality\n",
      "Quan\n",
      "NUMB\n",
      "ORD\n",
      "CARD\n",
      "FREQ\n",
      "DIST\n",
      "Time@\n",
      "TIME\n",
      "Space\n",
      "POS\n",
      "DIM\n",
      "Rel\n",
      "COLOR\n",
      "Self\n",
      "Our\n",
      "You\n",
      "Name\n",
      "Yes\n",
      "No\n",
      "Negate\n",
      "Intrj\n",
      "IAV\n",
      "DAV\n",
      "SV\n",
      "IPadj\n",
      "IndAdj\n",
      "PowGain\n",
      "PowLoss\n",
      "PowEnds\n",
      "PowAren\n",
      "PowCon\n",
      "PowCoop\n",
      "PowAuPt\n",
      "PowPt\n",
      "PowDoct\n",
      "PowAuth\n",
      "PowOth\n",
      "PowTot\n",
      "RcEthic\n",
      "RcRelig\n",
      "RcGain\n",
      "RcLoss\n",
      "RcEnds\n",
      "RcTot\n",
      "RspGain\n",
      "RspLoss\n",
      "RspOth\n",
      "RspTot\n",
      "AffGain\n",
      "AffLoss\n",
      "AffPt\n",
      "AffOth\n",
      "AffTot\n",
      "WltPt\n",
      "WltTran\n",
      "WltOth\n",
      "WltTot\n",
      "WlbGain\n",
      "WlbLoss\n",
      "WlbPhys\n",
      "WlbPsyc\n",
      "WlbPt\n",
      "WlbTot\n",
      "EnlGain\n",
      "EnlLoss\n",
      "EnlEnds\n",
      "EnlPt\n",
      "EnlOth\n",
      "EnlTot\n",
      "SklAsth\n",
      "SklPt\n",
      "SklOth\n",
      "SklTot\n",
      "TrnGain\n",
      "TrnLoss\n",
      "TranLw\n",
      "MeansLw\n",
      "EndsLw\n",
      "ArenaLw\n",
      "PtLw\n",
      "Nation\n",
      "Anomie\n",
      "NegAff\n",
      "PosAff\n",
      "SureLw\n",
      "If\n",
      "NotLw\n",
      "TimeSpc\n",
      "FormLw\n",
      "Othtags\n",
      "Defined\n"
     ]
    }
   ],
   "source": [
    "# Look at all data fields available\n",
    "for column in db_sentiment.columns:\n",
    "    print column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** some columns seem quite interesting for analysing relationship other than simply positive or negative sentiment (e.g. \"causal\", etc.). For the baseline, we will only use the \"positive\" and \"negative\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter only the words labeled positive or negative\n",
    "positive = db_sentiment[db_sentiment.Positiv == \"Positiv\"].Entry.map(lambda x: x.lower()).tolist()\n",
    "negative = db_sentiment[db_sentiment.Negativ == \"Negativ\"].Entry.map(lambda x: x.lower()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abide', u'ability', u'able', u'abound']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abandon', u'abandonment', u'abate', u'abdicate']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform list into sets for faster search\n",
    "positive = set(positive)\n",
    "negative = set(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an attempt to classify whether a sentence is positive or negative.  \n",
    "Note the main weaknesses:  \n",
    "1. It is \"positively\" biased for now since looks at the positive words first and if it finds it, then it immediately returns positive. Thus, it may not look at the entire sentence in case of both positive or negative words.  \n",
    "2. Negation of a positive word is not taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def includeSentiment(sentence, poslist, neglist): \n",
    "    ''' Classify sentence as positive or negative based on first word found\n",
    "        in the lexicon\n",
    "    '''\n",
    "    if any(word in sentence for word in poslist):\n",
    "        return (\"positive\", sentence)\n",
    "    elif any(word in sentence for word in neglist):\n",
    "        return (\"negative\", sentence)\n",
    "    else:\n",
    "        return (\"neutral\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add sentiment as key in the RDD\n",
    "sentiments = sentences.map(lambda a: includeSentiment(a, positive, negative))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positive',\n",
       "  u'in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.'),\n",
       " ('positive',\n",
       "  u'digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.')]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at 2 first lines\n",
    "sentiments.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 2 lists of filtered sentences: one positive list and one negative list\n",
    "pos_sentiments = sentiments.lookup(\"positive\")\n",
    "neg_sentiments = sentiments.lookup(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.',\n",
       " u'digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.',\n",
       " u'the results introduce, for the first time, new potent ACEI-inhibitory peptides that can be released by gastric pepsin of goat milk whey and caseins and thus may pave the way for their candidacy as anti-hypertensive bioactive peptides and prevention of associated disorders.',\n",
       " u'ACEI use between 1 january 2003 and the index date were determined by the date of hospitalization for acute pancreatitis among the cases.',\n",
       " u'this meta-analysis of randomized parallel controlled trials was designed to compare the efficacy of atenolol with ACEI in changing pulse wave velocity (pwv), peripheral blood pressure and heart rate (hr) among patients with essential hypertension.',\n",
       " u'enzymatic hydrolysis of proteins from rice, soy, pea and wheat, with both chymotrypsin and thermolysin, resulted in hydrolysates, which are efficient inhibitors of the ACEI (ACEI).',\n",
       " u'compared to hydrolysates from whey protein, where the inhibitory effect can almost exclusively be attributed to ile-trp, the ACEI inhibition by plant protein hydrolysates is caused by a variety of peptides, in particular tyrosine-containing peptides.',\n",
       " u'the annual rate of adverse events related to ACEI (ie, the number of reported cases of adverse events per 1000 patients receiving an ACEI) was calculated from data captured on the date the events were first reported for the 5 years before and 5 years after the revocation of the pa constraint.',\n",
       " u'in multivariate analysis, a past history of pneumonia (ahr 1.95, 95% ci: 1.35-2.8), chronic pulmonary disease (ahr 1.86, 1.24-2.78) and inhaled corticosteroid usage (ahr 1.78, 1.12-2.84) and hypnotic/sedative medication usage (ahr 2.06, 1.28-3.31) were identified as independent risk factors for recurrent pneumonia, whereas ACEI-inhibitors usage was associated with a reduction of the risk of rp (ahr 0.22, 0.05-0.91).']"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of sentences with positive sentiment lexicon\n",
    "pos_sentiments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the late-eluting fraction (f4) of either whey or caseins exhibited greater ACEI inhibition.',\n",
       " u'ACEI inhibition relies on the formation of hydrogen bonds between c-terminal residues of lentil peptides and residues of the ACEI catalytic site.']"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of sentences with negative sentiment lexicon\n",
    "neg_sentiments[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findSentiment(sentence, sentiment, poslist, neglist):\n",
    "    '''Print out the lexicon word that classified the sentence as positive or negative\n",
    "    '''\n",
    "    if sentiment == \"positive\":\n",
    "        lexicon = poslist\n",
    "    else:\n",
    "        lexicon = neglist\n",
    "    for word in lexicon:\n",
    "        # Note: had to use this \"try/except\" since there was an unicode ascii error... any ways\n",
    "        # to fix this without this try/except? if left the same, then we won't be able to see\n",
    "        # some sentiment word in some sentences.\n",
    "        try:\n",
    "            if word in sentence:\n",
    "                print word\n",
    "        except:\n",
    "            next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "oat\n",
      "whey\n",
      "pro\n",
      "generate\n",
      "significant\n"
     ]
    }
   ],
   "source": [
    "s5 = \"digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins\"\n",
    "findFoodItems(s5, foodlist)\n",
    "findSentiment(s5, \"positive\", positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "whey\n",
      "inhibit\n",
      "inhibition\n"
     ]
    }
   ],
   "source": [
    "s6 = \"the late-eluting fraction (f4) of either whey or caseins exhibited greater ACEI inhibition\"\n",
    "findFoodItems(s6, foodlist)\n",
    "findSentiment(s6, \"negative\", positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTags(sentence, sentiment, foods, poslist, neglist):\n",
    "    ''' Returns the tags of the sentence \n",
    "        Both lexicon word that classified the sentence as positive or negative and food name\n",
    "    '''\n",
    "    \n",
    "    sent = []\n",
    "    food = []\n",
    "    \n",
    "    if sentiment == \"positive\":\n",
    "        lexicon = poslist\n",
    "    else:\n",
    "        lexicon = neglist\n",
    "    for word in lexicon:\n",
    "        # Note: had to use this \"try/except\" since there was an unicode ascii error... any ways\n",
    "        # to fix this without this try/except? if left the same, then we won't be able to see\n",
    "        # some sentiment word in some sentences.\n",
    "        try:\n",
    "            if word in sentence:\n",
    "                sent.append(word)\n",
    "        except:\n",
    "            next\n",
    "            \n",
    "    # If using simple test of if food name \"in\" sentence method\n",
    "    #for f in foods:\n",
    "    #    if f in sentence:\n",
    "    #        food.append(f)\n",
    "    \n",
    "    # If using string distance method:\n",
    "    for f in foods:\n",
    "        n = min(3, len(f.split()))  # Assuming max as trigram        \n",
    "        try:\n",
    "            sentence = sentence.encode(\"utf-8\")\n",
    "            sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                    # last word. For now ok, since the JW will still be > 0.95\n",
    "            for ngram in sentence_ngrams:\n",
    "                # Note: when using jaro_winkler, need to convert into unicode format\n",
    "                if jellyfish.jaro_winkler(u\"{}\".format(f.lower()), u\"{}\".format(ngram.lower())) > 0.95:\n",
    "                    food.append(f)\n",
    "                    \n",
    "        except:\n",
    "            next\n",
    "\n",
    "\n",
    "            \n",
    "    return [sent, food, sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save results in a text file\n",
    "# Note: this could have been also done in Spark! But felt lazy to code... feel free to try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Positive.txt\", \"w\") as pos:\n",
    "    for sentence in pos_sentiments:\n",
    "        tags = findTags(sentence, \"positive\", foodlist, positive, negative)\n",
    "        pos.writelines(str(tags)+ \"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Negative.txt\", \"w\") as pos:\n",
    "    for sentence in pos_sentiments:\n",
    "        tags = findTags(sentence, \"negative\", foodlist, positive, negative)\n",
    "        pos.writelines(str(tags)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes**  \n",
    "1. In our baseline, some words like \"date\" that appear in a sentence will be interpreted as the fruit \"date\" instead of a calendar date and thus, will be filtered as outputs sentences. This can only be solved if we take into account the context of the sentence and we will need ML to model this!  \n",
    "2. Sentiment analysis need a major improvement: only basing on the positive and negative words without how the food and drug are connected through these words is not a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
