{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline : Food-drug Interaction Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Jul  2 2016 17:42:40)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import sys \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import codecs\n",
    "import jellyfish\n",
    "from collections import defaultdict\n",
    "\n",
    "spark_home = os.environ['SPARK_HOME'] = \\\n",
    "   '/home/adam/spark-2.0.1-bin-hadoop2.7'\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python2.7\"\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.10.3-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))\n",
    "app_name = \"w266-project\"\n",
    "    \n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "    Et je m'en vais\n",
      "    Au vent mauvais\n",
      "        Qui m'emporte\n",
      "\n",
      "    Deçà, delà,\n",
      "    Pareil à la\n",
      "        Feuille morte\n"
     ]
    }
   ],
   "source": [
    "# unicode test\n",
    "print ''' \n",
    "    Et je m'en vais\n",
    "    Au vent mauvais\n",
    "        Qui m'emporte\n",
    "\n",
    "    Deçà, delà,\n",
    "    Pareil à la\n",
    "        Feuille morte'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load food names or compounds into a list of unique items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pickling the data from foodb.ca database, see **compound_food_id.ipynb**  \n",
    "\n",
    "As a first test, we will use only the food common name (not scientific name) only. Compounds names will be added once this test passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# food_common.pickle: Dictionary with common English food names as keys, compounds as values\n",
    "test = pickle.load(open( \"data/food_common.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foodlist = []\n",
    "for food in test.keys():\n",
    "    foodlist.append(food.decode('utf-8').lower())\n",
    "foodlist = set(foodlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Filter sentences from abstract with drug keyword and food names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Test with only the abstracts' first json file. Once works, we can add all 100 remaining files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark imitator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/home/adam/anaconda3/envs/python2/bin/python\n",
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import sys \n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import codecs\n",
    "import jellyfish\n",
    "from collections import defaultdict\n",
    "import pubmed.utils as pb\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------- helper functions --------------------------- #\n",
    "def list_loader(data):\n",
    "    test = pickle.load(open( data , \"rb\"))\n",
    "    foodlist = []\n",
    "    for food in test.keys():\n",
    "        foodlist.append(food.decode('utf-8').lower())\n",
    "    return set(foodlist)\n",
    "\n",
    "def abstract_loader(name):\n",
    "    with codecs.open(name,\"r\",\"utf-8\") as data_file:\n",
    "        data = json.load(data_file)        \n",
    "    return data.itervalues()\n",
    "\n",
    "def splitSentences(abstract):\n",
    "    sentences = sent_tokenize(abstract)\n",
    "    return sentences\n",
    "\n",
    "def flat_map(sentences):\n",
    "    return [sent for s in sentences for sent in s]\n",
    "\n",
    "def find_ngrams(sentence, n):\n",
    "    ''' Return list of ngrams from a sentence\n",
    "    '''\n",
    "    words_list = sentence.split()\n",
    "    ngrams = zip(*[words_list[i:] for i in range(n)])\n",
    "    return [''.join([unicode(w)+' ' for w in ngram if type(w)==unicode]).strip() for ngram in ngrams]\n",
    "\n",
    "def spark_imitator(abstracts, drugkeyword, distance):\n",
    "    split_sentences = map(lambda abstract: splitSentences(abstract), abstracts)\n",
    "    flat_sentences = flat_map(split_sentences)\n",
    "    substituted_sentences = map(lambda sentence, drugkeyword: pb.ace_substitutor(sentence, drugkeyword), flat_sentences,\n",
    "                                drugkeyword)\n",
    "    drug_filtered_sentences = filter(lambda sentence: drugkeyword in sentence, substituted_sentences)\n",
    "    food_filtered_sentences = filter(lambda sentence: includeFoodCmpd(sentence, foodlist, distance), drug_filtered_sentences)\n",
    "    return food_filtered_sentences\n",
    "\n",
    "def includeFoodCmpd(sentence, fdlist, distance, verbose = False):\n",
    "    ''' Calculates the Jaro Wrinkler distance between food name and ngrams in the sentence.\n",
    "        Returns True if distance > 0.95\n",
    "    '''\n",
    "    result = False\n",
    "    for food in fdlist:\n",
    "        n = min(3, len(food.split()))  # Assuming max as trigram        \n",
    "        sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                    # last word. For now ok, since the JW will still be > 0.95\n",
    "        for ngram in sentence_ngrams:\n",
    "            ngram_distance = jellyfish.jaro_winkler(food.lower(), ngram.lower())\n",
    "            if verbose:\n",
    "                print food, ngram, ngram_distance\n",
    "            if ngram_distance > distance: \n",
    "                result = True\n",
    "                break\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ------------------ input settings ------------------ #\n",
    "    NAME = \"../analysis/pbabstract\"+ sys.argv[1] +  \".json\"\n",
    "    FOODLIST = \"data/\" + sys.argv[2] + \".pickle\" \n",
    "    DRUGKEYWORD = sys.argv[3]\n",
    "\n",
    "    \n",
    "    # ------------------------ run ----------------------- #\n",
    "    foodlist = list_loader(FOODLIST)\n",
    "    abstracts = abstract_loader(NAME)\n",
    "    food_filtered_sentences = spark_imitator(abstracts, DRUGKEYWORD, 0.95)\n",
    "    print food_filtered_sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map abstracts to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting driver.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile driver.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# try 10 files\n",
    "for N in $(seq 1 10); do\n",
    "    ./mapper.py  $N 'food_common' 'ACEI'  > res.$N.txt  &\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py\n",
    "!chmod a+x driver.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check with 10 abstracts, the first 9 gets parsed but the 10th has some problem\n",
    "but it works and not so slow, 9 abstracts (with 1 failed, I will try to look into it why later) in 3min 15secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./mapper.py\", line 85, in <module>\n",
      "    food_filtered_sentences = spark_imitator(abstracts, DRUGKEYWORD, 0.95)\n",
      "  File \"./mapper.py\", line 44, in spark_imitator\n",
      "    split_sentences = map(lambda abstract: splitSentences(abstract), abstracts)\n",
      "  File \"./mapper.py\", line 44, in <lambda>\n",
      "    split_sentences = map(lambda abstract: splitSentences(abstract), abstracts)\n",
      "  File \"./mapper.py\", line 30, in splitSentences\n",
      "    sentences = sent_tokenize(abstract)\n",
      "  File \"/home/adam/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tokenize/__init__.py\", line 91, in sent_tokenize\n",
      "    return tokenizer.tokenize(text)\n",
      "  File \"/home/adam/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tokenize/punkt.py\", line 1226, in tokenize\n",
      "    return list(self.sentences_from_text(text, realign_boundaries))\n",
      "  File \"/home/adam/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tokenize/punkt.py\", line 1274, in sentences_from_text\n",
      "    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n",
      "  File \"/home/adam/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tokenize/punkt.py\", line 1265, in span_tokenize\n",
      "    return [(sl.start, sl.stop) for sl in slices]\n",
      "  File \"/home/adam/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tokenize/punkt.py\", line 1304, in _realign_boundaries\n",
      "    for sl1, sl2 in _pair_iter(slices):\n",
      "  File \"/home/adam/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tokenize/punkt.py\", line 310, in _pair_iter\n",
      "    prev = next(it)\n",
      "  File \"/home/adam/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tokenize/punkt.py\", line 1278, in _slices_from_text\n",
      "    for match in self._lang_vars.period_context_re().finditer(text):\n",
      "TypeError: expected string or buffer\n",
      "CPU times: user 3.68 s, sys: 300 ms, total: 3.98 s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!./driver.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for pbabstract1.json we can do this:\n",
    "!./mapper.py 1 'food_common' 'ACEI'  > try1.txt\n",
    "# for pbasbtract2.json \n",
    "!./mapper.py 2 'food_common' 'ACEI'  > try2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"in this study, we examined the separated caseins and whey proteins of goat milk for the presence of ACEI inhibitory peptides.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findFoodItems(sentence, foods):\n",
    "    for item in foods:\n",
    "        if item in sentence:\n",
    "            print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "oat\n",
      "whey\n"
     ]
    }
   ],
   "source": [
    "findFoodItems(s, foodlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "oat\n",
      "whey\n"
     ]
    }
   ],
   "source": [
    "s2 = \"digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins.\"\n",
    "findFoodItems(s2, foodlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Classify sentences as positive or negative based on a sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment lexicon used is the Harvard General Inquirer (http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm). It contains 1,915 positive words and 2,291 negative words and is free for research use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/python2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (63,108,109,110,176) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "db_sentiment = pd.read_csv(\"data/inquirerbasic.csv\", encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Source</th>\n",
       "      <th>Positiv</th>\n",
       "      <th>Negativ</th>\n",
       "      <th>Pstv</th>\n",
       "      <th>Affil</th>\n",
       "      <th>Ngtv</th>\n",
       "      <th>Hostile</th>\n",
       "      <th>Strong</th>\n",
       "      <th>Power</th>\n",
       "      <th>...</th>\n",
       "      <th>Anomie</th>\n",
       "      <th>NegAff</th>\n",
       "      <th>PosAff</th>\n",
       "      <th>SureLw</th>\n",
       "      <th>If</th>\n",
       "      <th>NotLw</th>\n",
       "      <th>TimeSpc</th>\n",
       "      <th>FormLw</th>\n",
       "      <th>Othtags</th>\n",
       "      <th>Defined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DET ART</td>\n",
       "      <td>| article: Indefinite singular article--some o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ngtv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>H4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noun</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABATE</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABATEMENT</td>\n",
       "      <td>Lvd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Noun</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entry Source Positiv  Negativ Pstv Affil  Ngtv Hostile Strong Power  \\\n",
       "0            A  H4Lvd     NaN      NaN  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "1      ABANDON  H4Lvd     NaN  Negativ  NaN   NaN  Ngtv     NaN    NaN   NaN   \n",
       "2  ABANDONMENT     H4     NaN  Negativ  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "3        ABATE  H4Lvd     NaN  Negativ  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "4    ABATEMENT    Lvd     NaN      NaN  NaN   NaN   NaN     NaN    NaN   NaN   \n",
       "\n",
       "                         ...                         Anomie NegAff PosAff  \\\n",
       "0                        ...                            NaN    NaN    NaN   \n",
       "1                        ...                            NaN    NaN    NaN   \n",
       "2                        ...                            NaN    NaN    NaN   \n",
       "3                        ...                            NaN    NaN    NaN   \n",
       "4                        ...                            NaN    NaN    NaN   \n",
       "\n",
       "  SureLw   If NotLw TimeSpc FormLw  Othtags  \\\n",
       "0    NaN  NaN   NaN     NaN    NaN  DET ART   \n",
       "1    NaN  NaN   NaN     NaN    NaN     SUPV   \n",
       "2    NaN  NaN   NaN     NaN    NaN     Noun   \n",
       "3    NaN  NaN   NaN     NaN    NaN     SUPV   \n",
       "4    NaN  NaN   NaN     NaN    NaN     Noun   \n",
       "\n",
       "                                             Defined  \n",
       "0  | article: Indefinite singular article--some o...  \n",
       "1                                                  |  \n",
       "2                                                  |  \n",
       "3                                                  |  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 186 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Positiv</th>\n",
       "      <th>Causal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ACCOUNTABLE</td>\n",
       "      <td>Positiv</td>\n",
       "      <td>Causal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>FEASIBLE</td>\n",
       "      <td>Positiv</td>\n",
       "      <td>Causal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>FERTILE</td>\n",
       "      <td>Positiv</td>\n",
       "      <td>Causal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5290</th>\n",
       "      <td>IMPETUS</td>\n",
       "      <td>Positiv</td>\n",
       "      <td>Causal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5412</th>\n",
       "      <td>INDICATIVE</td>\n",
       "      <td>Positiv</td>\n",
       "      <td>Causal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry  Positiv  Causal\n",
       "95    ACCOUNTABLE  Positiv  Causal\n",
       "3973     FEASIBLE  Positiv  Causal\n",
       "4018      FERTILE  Positiv  Causal\n",
       "5290      IMPETUS  Positiv  Causal\n",
       "5412   INDICATIVE  Positiv  Causal"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To be continued... Notice that we do have some relationship words that are positive AND causaul\n",
    "db_sentiment[['Entry','Positiv','Causal']].sort_values(by=['Positiv','Causal']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Positive Words:  Positiv    1915\n",
      "Name: Positiv, dtype: int64\n",
      "Number of Negative Words:  Negativ    2291\n",
      "Name: Negativ, dtype: int64\n",
      "Number of Causaul Words:  Causal    112\n",
      "Name: Causal, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print \"Number of Positive Words: \", db_sentiment['Positiv'].value_counts()\n",
    "print \"Number of Negative Words: \", db_sentiment['Negativ'].value_counts()\n",
    "print \"Number of Causaul Words: \", db_sentiment['Causal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Look at all data fields available\n",
    "# for column in db_sentiment.columns:\n",
    "#     print column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** some columns seem quite interesting for analysing relationship other than simply positive or negative sentiment (e.g. \"causal\", etc.). For the baseline, we will only use the \"positive\" and \"negative\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter only the words labeled positive or negative\n",
    "positive = db_sentiment[db_sentiment.Positiv == \"Positiv\"].Entry.map(lambda x: x.lower()).tolist()\n",
    "negative = db_sentiment[db_sentiment.Negativ == \"Negativ\"].Entry.map(lambda x: x.lower()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abide', u'ability', u'able', u'abound']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abandon', u'abandonment', u'abate', u'abdicate']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform list into sets for faster search\n",
    "positive = set(positive)\n",
    "negative = set(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an attempt to classify whether a sentence is positive or negative.  \n",
    "Note the main weaknesses:  \n",
    "1. It is \"positively\" biased for now since looks at the positive words first and if it finds it, then it immediately returns positive. Thus, it may not look at the entire sentence in case of both positive or negative words.  \n",
    "2. Negation of a positive word is not taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def includeSentiment(sentence, poslist, neglist): \n",
    "    ''' Classify sentence as positive or negative based on first word found\n",
    "        in the lexicon\n",
    "    '''\n",
    "    if any(word in sentence for word in poslist):\n",
    "        return (\"positive\", sentence)\n",
    "    elif any(word in sentence for word in neglist):\n",
    "        return (\"negative\", sentence)\n",
    "    else:\n",
    "        return (\"neutral\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add sentiment as key in the RDD\n",
    "sentiments = sentences.map(lambda a: includeSentiment(a, positive, negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positive',\n",
       "  u'this meta-analysis of randomized parallel controlled trials was designed to compare the efficacy of atenolol with ACEI in changing pulse wave velocity (pwv), peripheral blood pressure and heart rate (hr) among patients with essential hypertension.'),\n",
       " ('negative',\n",
       "  u'using the ualdo:c and a relatively stringent definition of abt, it appears that incomplete raas blockade is common in dogs with mmvd receiving an ACEI.')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at 2 first lines\n",
    "sentiments.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 2 lists of filtered sentences: one positive list and one negative list\n",
    "pos_sentiments = sentiments.lookup(\"positive\")\n",
    "neg_sentiments = sentiments.lookup(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'this meta-analysis of randomized parallel controlled trials was designed to compare the efficacy of atenolol with ACEI in changing pulse wave velocity (pwv), peripheral blood pressure and heart rate (hr) among patients with essential hypertension.']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of sentences with positive sentiment lexicon\n",
    "pos_sentiments[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'using the ualdo:c and a relatively stringent definition of abt, it appears that incomplete raas blockade is common in dogs with mmvd receiving an ACEI.',\n",
       " u'we investigated the molecular mechanisms involved in the ACEI (ACEI) inhibition by (-)-epigallocatechin-3-gallate (egcg), a major tea catechin.']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of sentences with negative sentiment lexicon\n",
    "neg_sentiments[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findSentiment(sentence, sentiment, poslist, neglist):\n",
    "    '''Print out the lexicon word that classified the sentence as positive or negative\n",
    "    '''\n",
    "    if sentiment == \"positive\":\n",
    "        lexicon = poslist\n",
    "    else:\n",
    "        lexicon = neglist\n",
    "    for word in lexicon:\n",
    "        # Note: had to use this \"try/except\" since there was an unicode ascii error... any ways\n",
    "        # to fix this without this try/except? if left the same, then we won't be able to see\n",
    "        # some sentiment word in some sentences.\n",
    "        try:\n",
    "            if word in sentence:\n",
    "                print word\n",
    "        except:\n",
    "            next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "oat\n",
      "whey\n",
      "pro\n",
      "generate\n",
      "significant\n"
     ]
    }
   ],
   "source": [
    "s5 = \"digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins\"\n",
    "findFoodItems(s5, foodlist)\n",
    "findSentiment(s5, \"positive\", positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "whey\n",
      "inhibit\n",
      "inhibition\n"
     ]
    }
   ],
   "source": [
    "s6 = \"the late-eluting fraction (f4) of either whey or caseins exhibited greater ACEI inhibition\"\n",
    "findFoodItems(s6, foodlist)\n",
    "findSentiment(s6, \"negative\", positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTags(sentence, sentiment, foods, poslist, neglist):\n",
    "    ''' Returns the tags of the sentence \n",
    "        Both lexicon word that classified the sentence as positive or negative and food name\n",
    "    '''\n",
    "    \n",
    "    sent = []\n",
    "    food = []\n",
    "    \n",
    "    if sentiment == \"positive\":\n",
    "        lexicon = poslist\n",
    "    else:\n",
    "        lexicon = neglist\n",
    "    for word in lexicon:\n",
    "        # Note: had to use this \"try/except\" since there was an unicode ascii error... any ways\n",
    "        # to fix this without this try/except? if left the same, then we won't be able to see\n",
    "        # some sentiment word in some sentences.\n",
    "        try:\n",
    "            if word in sentence:\n",
    "                sent.append(word)\n",
    "        except:\n",
    "            next\n",
    "            \n",
    "    # If using simple test of if food name \"in\" sentence method\n",
    "    #for f in foods:\n",
    "    #    if f in sentence:\n",
    "    #        food.append(f)\n",
    "    \n",
    "    # If using string distance method:\n",
    "    for f in foods:\n",
    "        n = min(3, len(f.split()))  # Assuming max as trigram        \n",
    "        try:\n",
    "            sentence = sentence.encode(\"utf-8\")\n",
    "            sentence_ngrams = find_ngrams(sentence, n)  # Note: punctuation at end of sentence will be included with\n",
    "                                                    # last word. For now ok, since the JW will still be > 0.95\n",
    "            for ngram in sentence_ngrams:\n",
    "                # Note: when using jaro_winkler, need to convert into unicode format\n",
    "                if jellyfish.jaro_winkler(u\"{}\".format(f.lower()), u\"{}\".format(ngram.lower())) > 0.95:\n",
    "                    food.append(f)\n",
    "                    \n",
    "        except:\n",
    "            next\n",
    "\n",
    "\n",
    "            \n",
    "    return [sent, food, sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save results in a text file\n",
    "# Note: this could have been also done in Spark! But felt lazy to code... feel free to try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/Positive.txt\", \"w\") as pos:\n",
    "    for sentence in pos_sentiments:\n",
    "        tags = findTags(sentence, \"positive\", foodlist, positive, negative)\n",
    "        pos.writelines(str(tags)+ \"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/Negative.txt\", \"w\") as pos:\n",
    "    for sentence in pos_sentiments:\n",
    "        tags = findTags(sentence, \"negative\", foodlist, positive, negative)\n",
    "        pos.writelines(str(tags)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes**  \n",
    "1. In our baseline, some words like \"date\" that appear in a sentence will be interpreted as the fruit \"date\" instead of a calendar date and thus, will be filtered as outputs sentences. This can only be solved if we take into account the context of the sentence and we will need ML to model this!  \n",
    "2. Sentiment analysis need a major improvement: only basing on the positive and negative words without how the food and drug are connected through these words is not a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## **Slight modification to sentiment analysis**\n",
    "\n",
    "I wanted to see if making tuples of the words found in the sentences would help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findFoods(sentence, foods):\n",
    "    '''Making this a generator function'''\n",
    "    for item in foods:\n",
    "        if item in sentence:\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxSentiment(sentence, poslist, neglist): \n",
    "    ''' Count the number of positive and negative words in the sentence to ascertain\n",
    "    the type of sentence.\n",
    "    '''\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in poslist:\n",
    "            pos += 1\n",
    "        elif word in neglist:\n",
    "            neg += 1\n",
    "        else:\n",
    "            next\n",
    "    if pos > neg:\n",
    "        return ('positive',sentence)\n",
    "    elif neg > pos:\n",
    "        return ('negative',sentence)\n",
    "    else:\n",
    "        return ('neutral',sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', 'this is bad and terrible and also good')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxSentiment('this is bad and terrible and also good', positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "oat\n",
      "whey\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('negative',\n",
       " 'digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s5 = \"digestion of isolated whey proteins and caseins of goat milk by gastric pepsin generated soluble hydrolysates exhibiting significant inhibition of ACEI compared to weak inhibition by undigested proteins\"\n",
    "findFoodItems(s5, foodlist)\n",
    "maxSentiment(s5, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casein\n",
      "whey\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('negative',\n",
       " 'the late-eluting fraction (f4) of either whey or caseins exhibited greater ACEI inhibition')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s6 = \"the late-eluting fraction (f4) of either whey or caseins exhibited greater ACEI inhibition\"\n",
    "findFoodItems(s6, foodlist)\n",
    "maxSentiment(s6, positive, negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to look at pairs of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s5a = 'oat flakes are generally positively correlated with ACEI activity'\n",
    "s5b = 'this is a dummy sentence that will not even show up in the dictionary'\n",
    "s6a = 'oats and milk cause inhibition of ACEI'\n",
    "s6b = 'milk upregulates ACEI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def food_dict_maker(sentence):\n",
    "    '''Creating a dictionary of the positive and negative relationships between foods and the drug of choice'''\n",
    "    food_dict = defaultdict(list)\n",
    "\n",
    "    #Get the food item from generator function!\n",
    "    for item in list(findFoods(sentence,foodlist)):\n",
    "        try:\n",
    "            food_dict[(item, maxSentiment(sentence, positive, negative)[0])] += 1\n",
    "        except:\n",
    "            food_dict[(item, maxSentiment(sentence, positive, negative)[0])] = 1\n",
    "\n",
    "    yield food_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def food_dict_maker2(list_of_sentences):\n",
    "    '''Creating a dictionary of the positive and negative relationships between foods and the drug of choice'''\n",
    "    #food_dict = defaultdict(list)\n",
    "    food_dict = defaultdict(dict)\n",
    "\n",
    "    #Get the food item from generator function!\n",
    "    for sentence in list_of_sentences:\n",
    "        for item in list(findFoods(sentence,foodlist)):\n",
    "            try:\n",
    "                #food_dict[(item, maxSentiment(sentence, positive, negative)[0])] += 1\n",
    "                food_dict[item][maxSentiment(sentence, positive, negative)[0]] += 1\n",
    "            except:\n",
    "                #food_dict[(item, maxSentiment(sentence, positive, negative)[0])] = 1\n",
    "                food_dict[item][maxSentiment(sentence, positive, negative)[0]] = 1\n",
    "\n",
    "    yield food_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " defaultdict(<type 'dict'>, {'oat': {'neutral': 1, 'negative': 2}, 'whey': {'negative': 2}, 'casein': {'negative': 2}})\n"
     ]
    }
   ],
   "source": [
    "#The idea would be to pass in a TON of sentences here and you get a dictionary of dictionaries!\n",
    "#You're obviously still getting the error of 'oat' with 'goat' though :(\n",
    "for item in food_dict_maker2([s5a, s5b, s5, s6, s6a, s6b]):\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is what you could do with this dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_dict = {'oat': {'neutral': 1, 'negative': 2}, \n",
    "               'whey': {'negative': 2}, 'casein': {'negative': 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find relationships, use for viz\n",
    "sample_dict['oat']['negative']"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
